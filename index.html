<!doctype html>
<html lang="en-us">
<head>
  <title>Media Capture and Streams Extensions</title>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
  <script src="https://www.w3.org/Tools/respec/respec-w3c" class="remove"></script>
  <script class='remove'>
  "use strict";
  // See https://github.com/w3c/respec/wiki/ for how to configure ReSpec
  var respecConfig = {
    group: "webrtc",
    xref: ["geometry-1", "html", "infra", "permissions", "dom", "hr-time", "image-capture", "mediacapture-streams", "screen-capture", "webaudio", "webcodecs", "webidl"],
    edDraftURI: "https://w3c.github.io/mediacapture-extensions/",
    editors:  [
      {name: "Jan-Ivar Bruaroey", company: "Mozilla Corporation", w3cid: 79152},
      {name: "Elad Alon", company: "Google", email: "eladalon@google.com", w3cid: 118124},
    ],
    shortName: "mediacapture-extensions",
    specStatus: "ED",
    subjectPrefix: "[mediacapture-extensions]",
    github: "https://github.com/w3c/mediacapture-extensions",
  };
  </script>
</head>

<body>
  <section id="abstract">
    <p>This document defines a set of ECMAScript APIs in WebIDL to extend the [[mediacapture-streams]] specification.</p>
  </section>
  <section id="sotd">
    <p>This is an unofficial proposal.</p>
  </section>
  <section id="introduction">
    <h2>Introduction</h2>
    <p>This document contains proposed extensions and modifications to the
    [[mediacapture-streams]] specification.</p>
    <p>New features and modifications to existing features proposed here may be
    considered for addition into the main specification post Recommendation.
    Deciding factors will include maturity of the extension or modification,
    consensus on adding it, and implementation experience.</p>
    <p>A concrete long-term goal is reducing the fingerprinting surface of
    {{MediaDevices/enumerateDevices()}} by deprecating exposure of the device
    {{MediaDeviceInfo/label}} in its results. This requires relieving
    applications of the burden of building user interfaces to select cameras and
    microphones in-content, by offering this in user agents as part of
    {{MediaDevices/getUserMedia()}} instead.</p>
    <p>Miscellaneous other smaller features are under consideration as well,
    such as constraints to control multi-channel audio beyond stereo.</p>
  </section>
  <section>
    <h2>Terminology</h2>
    <p>
      This document uses the definitions {{MediaDevices}}, {{MediaStreamTrack}},
      {{MediaStreamConstraints}}, {{ConstrainablePattern}},
      {{MediaTrackSupportedConstraints}}, {{MediaTrackCapabilities}},
      {{MediaTrackConstraintSet}}, {{MediaTrackSettings}} and
      {{ConstrainBoolean}} from [[!mediacapture-streams]].
      <p>The terms [=permission state=], [=request permission to use=], and
      <a data-cite="permissions">prompt the user to choose</a> are defined in
      [[!permissions]].</p>
      <p>
        {{Performance.now()}} is defined in [[!hr-time]].
      </p>
  </section>
  <section id="conformance">
  </section>
  <section id="camera_and_microphone_picker">
    <h2>In-browser camera and microphone picker</h2>
    <p>The existing {{MediaDevices/enumerateDevices()}} function exposes camera
    and microphone {{MediaDeviceInfo/label}}s to let applications build
    in-content user interfaces for camera and microphone selection. Applications
    have had to do this because {{MediaDevices/getUserMedia()}} did not offer a
    web compatible in-agent device picker. This specification aims to rectify
    that.</p>
    <p>Due to the significant fingerprinting vector caused by device
    {{MediaDeviceInfo/label}}s, and the well-established nature of the existing
    APIs, the scope of this particular effort is limited to removing
    {{MediaDeviceInfo/label}}, leaving the overall constraints-based model
    intact. This helps ensure a migration path more viable than to a
    less-powerful API.</p>
    <p>This specification augments the existing {{MediaDevices/getUserMedia()}}
    function instead of introducing a new less-powerful API to compete with it,
    for that reason as well.</p>
    <section>
      <h3>getUserMedia "user-chooses" semantics</h3>
      <p>This specification introduces
      slightly altered semantics to the {{MediaDevices/getUserMedia()}}
      function called <code>"user-chooses"</code> that guarantee a picker will
      be shown to the user in cases where the user agent would otherwise choose
      for the user (that is: when application constraints do not narrow down
      the choices to a single device). This is orthogonal to permission, and
      offers a better and more consistent user experience across applications
      and user agents.
      </p>
      <p>Unfortunately, since the <code>"user-chooses"</code> semantics may
      produce user agent prompts at different times and in different situations
      compared to the old semantics, they are somewhat incompatible with
      expectations in some existing web applications that tend to call
      {{MediaDevices/getUserMedia()}} repeatedly and lazily instead of using
      e.g. <code>stream.clone()</code>.</p>
    </section>
    <section>
      <h3>Web compatibility and migration</h3>
      <p>User agents are encouraged to provide the new semantics as opt-in
      initially for web compatibility. User agents MUST deprecate (remove)
      {{MediaDeviceInfo/label}} from {{MediaDeviceInfo}} over time, though specific migration strategies
      are left to user agents. User agents SHOULD migrate to offering the new
      semantics by default (opt-out) over time.</p>
      <p>Since the constraints-model remains intact, web compatibility problems
      are expected to be limited to:</p>
      <ul>
        <li>
          Sites that never migrated show e.g. "Camera 1", "Camera 2" etc.
          instead of descriptive device labels
        </li>
        <li>
          Sites with no device management strategy provoke a picker in the
          user agent every visit for users with more than a singular choice
          of camera or microphone (a feature of sorts)
        </li>
      </ul>
    </section>
    <section id="mediadevices-interface">
      <h3>MediaDevices Interface Extensions</h3>
      <div>
        <pre class="idl"
>partial interface MediaDevices {
  readonly attribute GetUserMediaSemantics defaultSemantics;
};</pre>
      </div>
      <section>
        <h2>Attributes</h2>
        <dl data-link-for="MediaDevices" data-dfn-for="MediaDevices"
        class="attributes">
          <dt id="def-mediadevices-defaultSemantics"><dfn><code>defaultSemantics</code></dfn>
          of type <span class="idlAttrType"><a>GetUserMediaSemantics</a></span>, readonly</dt>
          <dd>
            <p>The default semantics of {{MediaDevices/getUserMedia()}} in this
            user agent.</p>
            <p>User agents SHOULD default to <code>"browser-chooses"</code>
            for backwards compatibility, until a transition plan has been
            enacted where a majority of user agents collectively switch their
            defaults to <code>"user-chooses"</code> for improved user privacy,
            and usage metrics suggest this transition is feasible without
            major breakage.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section id="mediastreamconstraints-dictionary-extensions">
      <h3>MediaStreamConstraints dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaStreamConstraints {
  GetUserMediaSemantics semantics;
};</pre>
        <section>
          <h2>Dictionary {{MediaStreamConstraints}} Members</h2>
          <dl data-link-for="MediaStreamConstraints" data-dfn-for=
          "MediaStreamConstraints" class="dictionary-members">
            <dt><dfn><code>semantics</code></dfn> of type <span class=
            "idlMemberType">{{GetUserMediaSemantics}}</span></dt>
            <dd>
              <p>In cases where the specified constraints do not narrow
              multiple choices between devices down to one per kind, specifies
              how the final determination of which devices to pick from the
              remaining choices MUST be made. If not specified, then the
              <a data-link-for="MediaDevices">defaultSemantics</a> are used.
              </p>
            </dd>
          </dl>
        </section>
      </div>
    </section>
    <section id="getusermediasemantics-enum">
      <h3>GetUserMediaSemantics enum</h3>
      <div>
        <pre class="idl"
>enum GetUserMediaSemantics {
  "browser-chooses",
  "user-chooses"
};</pre>
        <table data-link-for="GetUserMediaSemantics" data-dfn-for=
        "GetUserMediaSemantics" class="simple">
          <tbody>
            <tr>
              <th colspan="2"><dfn>GetUserMediaSemantics</dfn> Enumeration
              description</th>
            </tr>
            <tr>
              <td><dfn><code id=
              "idl-def-GetUserMediaSemantics.browser-chooses">browser-chooses</code></dfn></td>
              <td>
                <p>When application-specified constraints do not narrow multiple
                choices between devices down to one per kind, the user agent is
                allowed to make the final determination between the remaining
                choices.
                </p>
              </td>
            </tr>
            <tr>
              <td><dfn><code id=
              "idl-def-GetUserMediaSemantics.user-chooses">user-chooses</code></dfn></td>
              <td>
                <p>When application-specified constraints do not narrow
                multiple choices between devices down to one per kind, the user
                agent MUST
                <a href="prompt-the-user-to-choose">prompt the user to choose</a>
                between the remaining choices, even if the application already
                has permission to some or all of them.</p>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
    <section>
      <h3>Algorithms</h3>
      <p>When the {{MediaDevices/getUserMedia()}} method is invoked, run the
      following steps before invoking the {{MediaDevices/getUserMedia()}}
      algorithm:</p>
      <ol>
          <li>
            <p>Let <var>mediaDevices</var> be the object on which this method was
            invoked.</p>
          </li>
          <li>
            <p>Let <var>constraints</var> be the method's first argument.</p>
          </li>
          <li>
            <p>Let <var>semanticsPresent</var> be <code>true</code> if
            <var>constraints</var><code>.semantics</code> [= map/exists =],
            otherwise <code>false</code>.</p>
          </li>
          <li>
            <p>Let <var>semantics</var> be
            <var>constraints</var><code>.semantics</code>
            if it [= map/exists =],
            or the value of <var>mediaDevices</var><code>.<a data-link-for="MediaDevices">defaultSemantics</a></code>
            otherwise.</p>
          </li>
          <li>
            <p>Replace step 6.5.1. of the {{MediaDevices/getUserMedia()}}
            algorithm in its entirety with the following two steps:</p>
            <ol>
              <li>
                <p>Let <var>descriptor</var> be a {{PermissionDescriptor}}
                with its {{PermissionDescriptor/name}} member set to the permission name
                associated with <var>kind</var> (e.g. [="camera"=] for
                <code>"video"</code>, [="microphone"=] for <code>"audio"</code>).</p>
              </li>
              <li>
                <p>If the number of unique devices sourcing tracks of
                media type <var>kind</var> in <var>candidateSet</var>
                is greater than <code>1</code> and
                <var>semantics</var> is <code>"user-chooses"</code>,
                then <a>prompt the user to choose</a> a device with
                <var>descriptor</var>, resulting in provided media.
                Otherwise, <a>request permission to use</a> a
                device with <var>descriptor</var>, while considering
                all devices being attached to a live and
                <a>same-permission</a> MediaStreamTrack in the current
                [=browsing
                context=] to mean having permission status {{PermissionState/"granted"}},
                resulting in provided media.</p>
                <p><dfn>Same-permission</dfn> in this context means a
                {{MediaStreamTrack}} that required the same level of
                permission to obtain as what is being requested.</p>
                <p>When asking the user’s permission, the user agent
                MUST disclose whether permission will be granted only to
                the device chosen, or to all devices of that
                <var>kind</var>.</p>
                <p>Let <var>track</var> be the provided media, which
                MUST be precisely one track of type <var>kind</var> from
                <var>finalSet</var>. If <var>semantics</var> is
                <code>"browser-chooses"</code> then the decision of
                which track to choose from <var>finalSet</var> is up
                to the User Agent, which MAY use the value of the computed
                "fitness distance" from the <a href=
                "https://www.w3.org/TR/mediacapture-streams/#dfn-selectsettings">
                SelectSettings</a>
                algorithm, the value of <var>semanticsPresent</var>,
                or any other internally-available information about
                the devices, as inputs to its decision.
                If <var>semantics</var> is <code>"user-chooses"</code>,
                and the application has not narrowed down the choices
                to one, then the user agent MUST ask the user to make
                the final selection.</p>
                <p>Once selected, the source of the
                {{MediaStreamTrack}} MUST NOT change.</p>
                <p>User Agents are encouraged to default to or present
                a default choice based primarily on fitness distance,
                and secondarily on the user's primary or system default
                device for <var>kind</var> (when possible). User Agents
                MAY allow users to use any media source, including
                pre-recorded media files.</p>
              </li>
            </ol>
          </li>
      </ol>
    </section>
    <section>
      <h3>Examples</h3>
      <div>
        <p>This example shows a setup with a start button and a camera selector
        using the new semantics (microphone is not shown for brievity but is
        equivalent).</p>
        <pre class="example">
&lt;button id="start"&gt;Start&lt;/button&gt;
&lt;button id="chosenCamera" disabled&gt;Camera: none&lt;/button&gt;
&lt;script&gt;

let cameraTrack = null;

start.onclick = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: {deviceId: localStorage.cameraId}
    });
    setCameraTrack(stream.getVideoTracks()[0]);
  } catch (err) {
    console.error(err);
  }
}

chosenCamera.onclick = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: true,
      semantics: "user-chooses"
    });
    setCameraTrack(stream.getVideoTracks()[0]);
  } catch (err) {
    console.error(err);
  }
}

function setCameraTrack(track) {
  cameraTrack = track;
  const {deviceId, label} = track.getSettings();
  localStorage.cameraId = deviceId;
  chosenCamera.innerText = `Camera: ${label}`;
  chosenCamera.disabled = false;
}
&lt;/script&gt;
        </pre>
      </div>
    </section>
  </section>
  <section>
    <h2>MediaStreamTrack extensions</h2>
    <h3>Transferable MediaStreamTrack</h3>
    <div>
      <p>A {{MediaStreamTrack}} is a <a data-cite="!HTML/#transferable-objects">transferable object</a>.
      This allows manipulating real-time media outside the context it was requested or created in,
      for instance in workers or third-party iframes.</p>
      <p>To preserve the existing privacy and security infrastructure, in particular for capture tracks,
      the track source lifetime management remains tied to the context that created it.
      The transfer algorithm MUST ensure the following behaviors:</p>
      <p>
        <ol>
          <li><p>The context named <var>originalContext</var> that created a track named <var>originalTrack</var> remains in control
          of the <var>originalTrack</var> source, named <var>trackSource</var>, even when <var>originalTrack</var> is transferred into <var>transferredTrack</var>.</p>
          </li>
          <li>
            <p>In particular, <var>originalContext</var> remains the proxy to privacy indicators of <var>trackSource</var>.
            <var>transferredTrack</var> or any of its clones are considered as tracks using <var>trackSource</var>
            as if they were tracks created in and controlled by <var>originalContext</var>.</p>
          </li>
          <li><p>When <var>originalContext</var> goes away, <var>trackSource</var> gets ended, thus <var>transferredTrack</var> gets ended.</p></li>
          <li><p>When <var>originalContext</var> would have muted/unmuted <var>originalTrack</var>, <var>transferredTrack</var> gets muted/unmuted.</p></li>
          <li><p>If <var>transferredTrack</var> is cloned in <var>transferredTrackClone</var>, <var>transferredTrackClone</var> is tied to <var>trackSource</var>.
          It is not tied to <var>originalTrack</var> in any way.</p></li>
          <li><p>If <var>transferredTrack</var> is transferred into <var>transferredAgainTrack</var>, <var>transferredAgainTrack</var> is tied to <var>trackSource</var>.
          It is not tied to <var>transferredTrack</var> or <var>originalTrack</var> in any way.</p></li>
        </ol>
      </p>
    </div>
    <div>
      <p>The WebIDL changes to make the track transferable are the following:
      <pre class="idl"
>[Exposed=(Window,Worker), Transferable]
partial interface MediaStreamTrack {
};</pre>
    </div>
    <div>
      <p>At creation of a {{MediaStreamTrack}} object, called <var>track</var>, run the following steps:</p>
      <ol>
        <li><p>Initialize <var>track</var>.`[[IsDetached]]` to <code>false</code>.</p></li>
      </ol>
    </div>
    <div>
      <p>The {{MediaStreamTrack}} <a data-cite="!HTML/#transfer-steps">transfer steps</a>, given <var>value</var> and <var>dataHolder</var>, are:</p>
      <ol>
        <li><p>If <var>value</var>.`[[IsDetached]]` is <code>true</code>, throw a "DataCloneError" DOMException.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[id]]` to <var>value</var>.{{MediaStreamTrack/id}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[kind]]` to <var>value</var>.{{MediaStreamTrack/kind}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[label]]` to <var>value</var>.{{MediaStreamTrack/label}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[readyState]]` to <var>value</var>.{{MediaStreamTrack/readyState}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[enabled]]` to <var>value</var>.{{MediaStreamTrack/enabled}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[muted]]` to <var>value</var>.{{MediaStreamTrack/muted}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[source]]` to <var>value</var> underlying source.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[constraints]]` to <var>value</var> active constraints.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[contentHint]]` to <var>value</var> application-set content hint.</p></li>
        <li><p>Set <var>value</var>.`[[IsDetached]]` to <code>true</code>.</p></li>
        <li><p>Set <var>value</var>.<a data-cite="mediacapture-streams#dfn-readystate" data-link-type="attribute">[[\ReadyState]]</a> to {{MediaStreamTrackState/"ended"}} (without stopping the underlying source or firing an `ended` event).</p></li>
      </ol>
    </div>
    <div><p>{{MediaStreamTrack}} <a data-cite="!HTML/#transfer-receiving-steps">transfer-receiving steps</a>, given <var>dataHolder</var> and <var>track</var>, are:</p>
      <ol>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/id}} to <var>dataHolder</var>.`[[id]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/kind}} to <var>dataHolder</var>.`[[kind]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/label}} to <var>dataHolder</var>.`[[label]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/readyState}} to <var>dataHolder</var>.`[[readyState]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/enabled}} to <var>dataHolder</var>.`[[enabled]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/muted}} to <var>dataHolder</var>.`[[muted]]`.</p></li>
        <li><p>Set <var>track</var> application-set content hint to <var>dataHolder</var>.`[[contentHint]]`.</p></li>
        <li><p>[=Initialize the underlying source=] of <var>track</var> to <var>dataHolder</var>.`[[source]]`.</p></li>
        <li><p>Set <var>track</var>'s constraints to <var>dataHolder</var>.`[[constraints]]`.</p></li>
      </ol>
    </div>
    <div>
      <p>The underlying source is supposed to be kept alive between the transfer and transfer-receiving steps, or as long as the data holder is alive.
      In a sense, between these steps, the data holder is attached to the underlying source as if it was a track.</p>
    </div>
    <h3>MediaStreamTrack Statistics</h3>
    <div>
      <p>
        On microphone audio tracks, frame counters allow the application to tell
        the ratio of audio that is delivered as one quality indicator and the
        latency metrics measure the input delay from capture to application.
      </p>
      <p>
        On camera and screenshare video tracks, frame counters allow the
        application to tell what the frame rate is, which may be lower than the
        target {{MediaTrackSettings/frameRate}}. For example, if the track is
        sourced from a camera then the production of frames could be slowed down
        if it's dark or frames could be dropped if the system is CPU starved.
        This could impact the total number of frames produced by the source and
        impact how many frames are delivered, discarded or dropped for other
        reasons.
      </p>
    </div>
    <div>
      <pre class="idl"
>partial interface MediaStreamTrack {
  [SameObject] readonly attribute
      (MediaStreamTrackAudioStats or MediaStreamTrackVideoStats)? stats;
};
</pre>
    </div>
    <div>
      <p>Let the {{MediaStreamTrack}} have a
      <dfn class=export data-dfn-for="MediaStreamTrack">[[\Stats]]</dfn>
      internal slot initialized it to <code>null</code>, unless otherwise
      specified below.</p>
      <p>If the track's is of {{MediaStreamTrack/kind}} <code>"audio"</code>,
      run the following steps:</p>
      <ol>
        <li>
          <p>If the {{MediaStreamTrack}} is sourced from
          <code>getUserMedia()</code>, initialize {{MediaStreamTrack/[[Stats]]}}
          to a new instance of {{MediaStreamTrackAudioStats}} set up to expose
          audio stats for this {{MediaStreamTrack}}.</p>
        </li>
      </ol>
      <p>If the track's is of {{MediaStreamTrack/kind}} <code>"video"</code>,
      run the following steps:</p>
      <ol>
        <li>
          <p>If the {{MediaStreamTrack}} is sourced from
          <code>getUserMedia()</code> or <code>getDisplayMedia()</code>,
          initialize {{MediaStreamTrack/[[Stats]]}} to a new instance of
          {{MediaStreamTrackVideoStats}} set up to expose video stats for this
          {{MediaStreamTrack}}.</p>
        </li>
      </ol>
    </div>
    <section>
      <h4>Attributes</h4>
      <dl data-link-for="MediaStreamTrack" data-dfn-for="MediaStreamTrack"
      class="attributes">
        <dt>
          <dfn data-idl="">stats</dfn> of type ({{MediaStreamTrackAudioStats}}
          or {{MediaStreamTrackVideoStats}}), readonly
        </dt>
        <dd>
          <p>
            When this getter is called, the user agenst MUST run the following
            steps:
          </p>
          <ol>
            <li>
              <p>
                Let <var>track</var> be the {{MediaStreamTrack}} that this
                getter is called on.
              </p>
            </li>
            <li>
              <p>Return <var>track</var>.{{MediaStreamTrack/[[Stats]]}}.</p>
            </li>
          </ol>
        </dd>
      </dl>
    </section>
  </section>
    <section>
      <h4>The MediaStreamTrackAudioStats interface</h4>
      <div>
        <pre class="idl" data-cite="HR-TIME">[Exposed=Window]
interface MediaStreamTrackAudioStats {
  readonly attribute unsigned long long deliveredFrames;
  readonly attribute DOMHighResTimeStamp deliveredFramesDuration;
  readonly attribute unsigned long long totalFrames;
  readonly attribute DOMHighResTimeStamp totalFramesDuration;
  readonly attribute DOMHighResTimeStamp latency;
  readonly attribute DOMHighResTimeStamp averageLatency;
  readonly attribute DOMHighResTimeStamp minimumLatency;
  readonly attribute DOMHighResTimeStamp maximumLatency;
  undefined resetLatency();
  [Default] object toJSON();
};
        </pre>
        <div class="note">
          <p>The following metrics lack Working Group consensus:
          {{MediaStreamTrackAudioStats/deliveredFrames}},
          {{MediaStreamTrackAudioStats/deliveredFramesDuration}},
          {{MediaStreamTrackAudioStats/totalFrames}} and
          {{MediaStreamTrackAudioStats/totalFramesDuration}}. See <a
          href="https://github.com/w3c/mediacapture-extensions/issues/129">Issue
          #129</a>.</p>
        </div>
        <div>
          <p>The {{MediaStreamTrackAudioStats}} expose frame counters for the
          {{MediaStreamTrack}} that created it. For this track, the user agent
          is required to count each audio frame from its source as follows:</p>
          <ul>
            <li>
              <p>A frame is considered a <dfn data-lt="delivered audio frames">
              delivered audio frame</dfn> if it either was delivered to a sink
              or would have been delivered to a sink, if one was connected.</p>
            </li>
            <li>
              <p>The <dfn data-lt="delivered audio frames duration">delivered
              audio frames duration</dfn> is the total duration of all
              [= delivered audio frames =]. This measurement is incremented at
              the same time as [= delivered audio frames =] and is measured in
              milliseconds.</p>
            </li>
            <li>
              <p>An audio frame that is discarded because it cannot be delivered
              on time, or it cannot be delivered for any other reason, is
              considered <dfn data-lt="dropped audio frames">dropped</dfn>.</p>
            </li>
            <li>
              <p>The <dfn data-lt="dropped audio frames duration">dropped audio
              frames duration</dfn> is the total duration of all [= dropped
              audio frames =]. This measurement is incremented at the same time
              as [= dropped audio frames =] and is measured in milliseconds.</p>
            <div class="note">
              <p>If the track is unmuted and enabled, the counters increase as
              audio is produced by the capture device. If no audio is flowing,
              such as if the track is muted or disabled, then the counters do
              not increase.</p>
            </div>
            </li>
            <li>
              <p><dfn data-lt="input latency">Input latency</dfn> is the time,
              in milliseconds, between the point in time an audio input device
              has acquired a signal and the time it is available for
              consumption, which may include buffering by the user agent.</p>
              <p>The <dfn data-lt="latest input latency">latest input
              latency</dfn> is the latest available [= input latency =] as
              estimated between the track's input device and delivery to any of
              its sinks.</p>
              <div class="note">
                <p>The user agent updates its estimates at sufficient frequency
                to allow monitoring. The latency is representative of the
                experienced delay, but is not necessarily an exact measurement
                of the last individual audio frame that was delivered.</p>
                <p>A sink that consumes audio may add additional processing
                latency not included in this measurement, such as playout delay
                or encode time.</p>
              </div>
              <p>Every time the [= latest input latency =] measurement is
              updated, the user agent also updates its
              <dfn data-lt="average input latency">average input latency</dfn>,
              <dfn data-lt="minimum input latency">minimum input latency</dfn>
              and <dfn data-lt="maximum input latency">maximum input
              latency</dfn> which are the average, minimum and maximum observed
              measurements since the last <dfn data-lt="latency reset time">
              latency reset time</dfn>.
              </p>
            </li>
          </ul>
          <p>Let the {{MediaStreamTrackAudioStats}} have internal slots
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\DeliveredFrames]]</dfn>,
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\DeliveredFramesDuration]]</dfn>,
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\DroppedFrames]]</dfn>,
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\DroppedFramesDuration]]</dfn>,
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\Latency]]</dfn>,
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\AverageLatency]]</dfn>,
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\MinimumLatency]]</dfn>
          and
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\MaximumLatency]]</dfn>,
          initialized to 0.</p>
          <p>Let the {{MediaStreamTrackAudioStats}} also have internal slots
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\LastTask]]</dfn>
          and
          <dfn class=export data-dfn-for="MediaStreamTrackAudioStats">[[\LastExposureTime]]</dfn>,
          initialized to <code>undefined</code>.</p>
          <p>The <dfn data-lt="expose audio frame counters steps">expose audio
          frame counters steps</dfn> are the following:</p>
          <ul>
            <li>
              <p>Let <var>task</var> be the current [=task=].</p>
            </li>
            <li>
              <p>If {{MediaStreamTrackAudioStats/[[LastTask]]}} is equal to
              <var>task</var>, abort these steps.</p>
            </li>
            <li>
              <p>Set {{MediaStreamTrackAudioStats/[[LastTask]]}} to
              <var>task</var>.</p>
            </li>
            <li>
              <p>Set {{MediaStreamTrackAudioStats/[[DeliveredFrames]]}} to
              [= delivered audio frames =],
              set {{MediaStreamTrackAudioStats/[[DeliveredFramesDuration]]}}
              to [= delivered audio frames duration =],
              set {{MediaStreamTrackAudioStats/[[DroppedFrames]]}} to
              [= dropped audio frames =],
              set {{MediaStreamTrackAudioStats/[[DroppedFramesDuration]]}} to
              [= dropped audio frames duration =],
              set {{MediaStreamTrackAudioStats/[[Latency]]}} to the
              [= latest input latency =],
              set {{MediaStreamTrackAudioStats/[[AverageLatency]]}} to the
              [= average input latency =],
              set {{MediaStreamTrackAudioStats/[[MinimumLatency]]}} to the
              [= minimum input latency =] and
              set {{MediaStreamTrackAudioStats/[[MaximumLatency]]}} to the
              [= maximum input latency =].</p>
              <p>Set {{MediaStreamTrackAudioStats/[[LastExposureTime]]}} to
              reflect the time that these metrics were exposed.</p>
              <div class="note">
                <p>Only updating these counters once per [=task=] preserves the
                <a href="https://w3ctag.github.io/design-principles/#js-rtc">
                run-to-completion</a> semantics defined in
                [[API-DESIGN-PRINCIPLES]].</p>
              </div>
            </li>
          </ul>
        </div>
        <section>
          <h4>Attributes</h4>
          <dl data-link-for="MediaStreamTrackAudioStats"
          data-dfn-for="MediaStreamTrackAudioStats" class="attributes">
            <dt>
              <dfn data-idl="">deliveredFrames</dfn> of type <span class=
              "idlMemberType">unsigned long long, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose audio frame counters steps =]
                and return
                {{MediaStreamTrackAudioStats/[[DeliveredFrames]]}}.
              </p>
            </dd>
            <dt>
              <dfn data-idl="">deliveredFramesDuration</dfn> of type <span
              class="idlMemberType">DOMHighResTimeStamp, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose audio frame counters steps =]
                and return
                {{MediaStreamTrackAudioStats/[[DeliveredFramesDuration]]}}.
              </p>
            </dd>
            <dt>
              <dfn data-idl="">totalFrames</dfn> of type <span class=
              "idlMemberType">unsigned long long, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose audio frame counters steps =]
                and return the sum of
                {{MediaStreamTrackAudioStats/[[DeliveredFrames]]}} and
                {{MediaStreamTrackAudioStats/[[DroppedFrames]]}}.
              </p>
            </dd>
            <dt>
              <dfn data-idl="">totalFramesDuration</dfn> of type <span class=
              "idlMemberType">DOMHighResTimeStamp, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose audio frame counters steps =]
                and return the sum of
                {{MediaStreamTrackAudioStats/[[DeliveredFramesDuration]]}} and
                {{MediaStreamTrackAudioStats/[[DroppedFramesDuration]]}}.
              </p>
              <div class="note">
                <p>Because audio capture devices produce audio in real-time,
                audio frames may be dropped if not processed in a timely
                manner.</p>
                <p>The ratio of audio duration that was delivered, i.e. not
                dropped, can be calculated as
                {{MediaStreamTrackAudioStats/deliveredFramesDuration}} /
                {{MediaStreamTrackAudioStats/totalFramesDuration}}.</p>
              </div>
            </dd>
            <dt>
              <dfn data-idl="">latency</dfn> of type <span
              class="idlMemberType">DOMHighResTimeStamp, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose audio frame counters steps =]
                and return {{MediaStreamTrackAudioStats/[[Latency]]}}.
              </p>
            </dd>
            <dt>
              <dfn data-idl="">averageLatency</dfn> of type <span
              class="idlMemberType">DOMHighResTimeStamp, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose audio frame counters steps =]
                and return {{MediaStreamTrackAudioStats/[[AverageLatency]]}}.
              </p>
            </dd>
            <dt>
              <dfn data-idl="">minimumLatency</dfn> of type <span
              class="idlMemberType">DOMHighResTimeStamp, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose audio frame counters steps =]
                and return {{MediaStreamTrackAudioStats/[[MinimumLatency]]}}.
              </p>
            </dd>
            <dt>
              <dfn data-idl="">maximumLatency</dfn> of type <span
              class="idlMemberType">DOMHighResTimeStamp, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose audio frame counters steps =]
                and return {{MediaStreamTrackAudioStats/[[MaximumLatency]]}}.
              </p>
            </dd>
          </dl>
        </section>
        <section>
          <h4>Methods</h4>
          <dl data-link-for="MediaStreamTrackAudioStats"
          data-dfn-for="MediaStreamTrackAudioStats" class="methods">
            <dt>
              <dfn data-idl="">resetLatency</dfn>
            </dt>
            <dd>
              <p>When called, run the following steps:</p>
              <ul>
                <li>
                  <p>Run the [= expose audio frame counters steps =].</p>
                </li>
                <li>
                  <p>Set {{MediaStreamTrackAudioStats/[[AverageLatency]]}},
                  {{MediaStreamTrackAudioStats/[[MinimumLatency]]}} and
                  {{MediaStreamTrackAudioStats/[[MaximumLatency]]}} to
                  {{MediaStreamTrackAudioStats/[[Latency]]}}.</p>
                </li>
                <li>
                  <p>Set the [= latency reset time =] to
                  {{MediaStreamTrackAudioStats/[[LastExposureTime]]}}.</p>
                </li>
              </ul>
              </p>
            </dd>
            <dt>
              <dfn data-idl="">toJSON</dfn>
            </dt>
            <dd>
              <p>
                When called, run [[!WEBIDL]]'s [= default toJSON steps =].
              </p>
            </dd>
          </dl>
        </section>
      </div>
    </section>
    <section>
      <h4>The MediaStreamTrackVideoStats interface</h4>
      <div>
        <pre class="idl">[Exposed=Window]
interface MediaStreamTrackVideoStats {
  readonly attribute unsigned long long deliveredFrames;
  readonly attribute unsigned long long discardedFrames;
  readonly attribute unsigned long long totalFrames;
  [Default] object toJSON();
};
        </pre>
        <div>
          <p>The {{MediaStreamTrackVideoStats}} expose frame counters for the
          {{MediaStreamTrack}} that created it. For this track, the user agent
          is required to count each video frame from its source as follows:</p>
          <ul>
            <li>
              <p>A frame is considered a <dfn data-lt="delivered video frames">
              delivered video frame</dfn> if it either was delivered to a sink
              or would have been delivered to a sink, if one was connected. This
              is a subset of [= total video frames =] and it is incremented at
              the same time as [= total video frames =].</p>
            </li>
            <li>
              <p>A video frame is considered
              <dfn data-lt="discarded video frames">discarded</dfn> if it was
              discarded in order to achieve the target
              {{MediaTrackSettings/frameRate}}. This is a subset of
              [= total video frames =] and it is incremented at the same time as
              [= total video frames =].</p>
            </li>
            <li>
              <p>The <dfn data-lt="total video frames">total</dfn> number of
              frames that have been processed by this source, meaning it is
              known whether the frame was considered delivered, discarded or
              dropped for any other reason. The number of dropped frames for
              various unknown reasons can be calculated by subtracting
              [= delivered video frames =] and [= discarded video frames =] from
              [= total video frames =].</p>
              <div class="note">
                <p>If the track is unmuted and enabled and the source is backed
                by a camera, total frames is incremented by frames produced by
                the camera. If no frames are flowing, such as if the track is
                muted or disabled, then total frames does not increment.</p>
              </div>
            </li>
          </ul>
          <p>Let the {{MediaStreamTrackVideoStats}} have internal slots
          <dfn class=export data-dfn-for="MediaStreamTrackVideoStats">[[\DeliveredFrames]]</dfn>,
          <dfn class=export data-dfn-for="MediaStreamTrackVideoStats">[[\DiscardedFrames]]</dfn>
          and
          <dfn class=export data-dfn-for="MediaStreamTrackVideoStats">[[\TotalFrames]]</dfn>,
          initialized to 0.
          </p>
          <p>Let the {{MediaStreamTrackVideoStats}} also have an internal slot
          <dfn class=export data-dfn-for="MediaStreamTrackVideoStats">[[\LastTask]]</dfn>
          initialized to <code>null</code>.</p>
          <p>The <dfn data-lt="expose video frame counters steps">expose video
          frame counters steps</dfn> are the following:</p>
          <ul>
            <li>
              <p>Let <var>task</var> be the current [=task=].</p>
            </li>
            <li>
              <p>If {{MediaStreamTrackVideoStats/[[LastTask]]}} is equal to
              <var>task</var>, abort these steps.</p>
            </li>
            <li>
              <p>Set {{MediaStreamTrackVideoStats/[[LastTask]]}} to
              <var>task</var>.</p>
            </li>
            <li>
              <p>Set {{MediaStreamTrackVideoStats/[[DeliveredFrames]]}} to
              [= delivered video frames =],
              set {{MediaStreamTrackVideoStats/[[DiscardedFrames]]}} to
              [= discarded video frames =] and
              set {{MediaStreamTrackVideoStats/[[TotalFrames]]}} to
              [= total video frames =].
              </p>
              <div class="note">
                <p>Only updating these counters once per [=task=] preserves the
                <a href="https://w3ctag.github.io/design-principles/#js-rtc">
                run-to-completion</a> semantics defined in
                [[API-DESIGN-PRINCIPLES]].</p>
              </div>
            </li>
          </ul>
        </div>
        <section>
          <h4>Attributes</h4>
          <dl data-link-for="MediaStreamTrackVideoStats"
          data-dfn-for="MediaStreamTrackVideoStats" class="attributes">
            <dt>
              <dfn data-idl="">deliveredFrames</dfn> of type <span class=
              "idlMemberType">unsigned long long, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose video frame counters steps =]
                and return {{MediaStreamTrackVideoStats/[[DeliveredFrames]]}}.
              </p>
            </dd>
            <dt>
              <dfn data-idl="">discardedFrames</dfn> of type <span class=
              "idlMemberType">unsigned long long, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose video frame counters steps =]
                and return {{MediaStreamTrackVideoStats/[[DiscardedFrames]]}}.
              </p>
            </dd>
            <dt>
              <dfn data-idl="">totalFrames</dfn> of type <span class=
              "idlMemberType">unsigned long long, readonly</span>
            </dt>
            <dd>
              <p>
                Upon getting, run the [= expose video frame counters steps =]
                and return {{MediaStreamTrackVideoStats/[[TotalFrames]]}}.
              </p>
            </dd>
          </dl>
        </section>
        <section>
          <h4>Methods</h4>
          <dl data-link-for="MediaStreamTrackVideoStats"
          data-dfn-for="MediaStreamTrackVideoStats" class="methods">
            <dt>
              <dfn data-idl="">toJSON</dfn>
            </dt>
            <dd>
              <p>
                When called, run [[!WEBIDL]]'s [= default toJSON steps =].
              </p>
            </dd>
          </dl>
        </section>
      </div>
    </section>
  </section>
  <section>
    <h2>The powerEfficient constraint</h2>
    <section>
      <h3>MediaTrackSupportedConstraints dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaTrackSupportedConstraints {
  boolean powerEfficient = true;
};</pre>
        <section>
          <h2>Dictionary {{MediaTrackSupportedConstraints}} Members</h2>
          <dl data-link-for="MediaTrackSupportedConstraints" data-dfn-for=
          "MediaTrackSupportedConstraints" class="dictionary-members">
            <dt><dfn>powerEfficient</dfn> of type
            {{boolean}}, defaulting to <code>true</code></dt>
            <dd>See <a href=
            "#def-constraint-powerEfficient">
            powerEfficient</a> for details.</dd>
          </dl>
        </section>
      </div>
    </section>
    <section>
      <h3>MediaTrackCapabilities dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaTrackCapabilities {
  sequence&lt;boolean&gt; powerEfficient;
};</pre>
        <section>
          <h2>Dictionary {{MediaTrackCapabilities}} Members</h2>
          <dl data-link-for="MediaTrackCapabilities" data-dfn-for=
          "MediaTrackCapabilities" class="dictionary-members">
            <dt><dfn>powerEfficient</dfn> of type
            <code>sequence&lt;{{boolean}}&gt;</code></dt>
            <dd>
              <p>The source may operate in different configurations.
              If all configurations have the same power efficiency
              impact, a single <code>false</code> is reported.
              Otherwise, the source reports a list with
              both <code>true</code> and <code>false</code>
              as possible values. See <a href=
              "#def-constraint-powerEfficient">
              powerEfficient</a> for additional
              details.</p>
            </dd>
          </dl>
        </section>
      </div>
    </section>
    <section>
      <h3>MediaTrackSettings dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaTrackSettings {
  boolean powerEfficient;
};</pre>
        <section>
          <h2>Dictionary {{MediaTrackSettings}} Members</h2>
          <dl data-link-for="MediaTrackSettings" data-dfn-for=
          "MediaTrackSettings" class="dictionary-members">
            <dt><dfn>powerEfficient</dfn> of type
            {{boolean}}</dt>
            <dd>See <a href=
            "#def-constraint-powerEfficient">
            powerEfficient</a> for details.</dd>
      </section>
      </div>
    </section>
    <h2>Constrainable Properties</h2>
    <p>The constrainable properties in this document are defined below.</p>
    <table class="simple">
      <thead>
        <tr>
          <th>Property Name</th>
          <th>Values</th>
          <th>Notes</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><dfn id="def-constraint-powerEfficient">
            powerEfficient</dfn></td>
          <td>{{ConstrainBoolean}}</td>
          <td>
            <p>
            Cameras can often operate in different configurations.
            Configurations are typically selected based on constraints
            that are related to observable parameters like width or height.
            Configurations may have less directly observable characteristics:
            power consumption, low light sensitivity, fast autofocus...
            The powerEfficient constraint allows web applications to
            favor selection of configurations that consume less power.
            This may be useful for web applications that may use the camera
            for an extended amount of time, like video conference web applications.
            On the other hand, applications that may use the camera for a small amount
            of time may prefer to not use the powerEfficient constraint.
            This constraint is only applicable to camera sources.
            </p>
            <p>As a constraint, setting it to true instructs the user agent
            to prefer configuration that it considers power efficient.</p>
          </td>
          <div class="note">
          This constraint is not usable with getUserMedia as a mandatory
          constraint, since it's not in <a>allowed required constraints for
          device selection</a>. User Agents MAY use it for device selection
          though the current distance algorithm is not directly applicable
          as devices may have different power consumptions while having the
          same `powerEfficient` capabilities.
          </div>
        </tr>
      </tbody>
    </table>
  </section>
  <section>
    <h2>The powerEfficientPixelFormat constraint</h2>
    <section id="mediatracksupportedconstraints-dictionary-extensions">
      <div class="note">
        This constraint is somewhat redundant with the <a href=
        "#def-constraint-powerEfficient"> powerEfficient</a>
        constraint. It may be removed in a future version of this
        specification.
      </div>
      <h3>MediaTrackSupportedConstraints dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaTrackSupportedConstraints {
  boolean powerEfficientPixelFormat = true;
};</pre>
        <section>
          <h2>Dictionary {{MediaTrackSupportedConstraints}} Members</h2>
          <dl data-link-for="MediaTrackSupportedConstraints" data-dfn-for=
          "MediaTrackSupportedConstraints" class="dictionary-members">
            <dt><dfn>powerEfficientPixelFormat</dfn> of type
            {{boolean}}, defaulting to <code>true</code></dt>
            <dd>See <a href=
            "#def-constraint-powerEfficientPixelFormat">
            powerEfficientPixelFormat</a> for details.</dd>
          </dl>
        </section>
      </div>
    </section>
    <section id="mediatrackcapabilities-dictionary-extensions">
      <h3>MediaTrackCapabilities dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaTrackCapabilities {
  sequence&lt;boolean&gt; powerEfficientPixelFormat;
};</pre>
        <section>
          <h2>Dictionary {{MediaTrackCapabilities}} Members</h2>
          <dl data-link-for="MediaTrackCapabilities" data-dfn-for=
          "MediaTrackCapabilities" class="dictionary-members">
            <dt><dfn>powerEfficientPixelFormat</dfn> of type
            <code>sequence&lt;{{boolean}}&gt;</code></dt>
            <dd>
              <p>If the source only has power efficient pixel formats, a single
              <code>true</code> is reported. If the source only has power
              inefficient pixel formats, a single <code>false</code> is
              reported. If the script can control the feature, the source
              reports a list with both <code>true</code> and <code>false</code>
              as possible values. See <a href=
              "#def-constraint-powerEfficientPixelFormat">
              powerEfficientPixelFormat</a> for additional
              details.</p>
            </dd>
          </dl>
        </section>
      </div>
    </section>
    <section id="mediatracksettings-dictionary-extensions">
      <h3>MediaTrackSettings dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaTrackSettings {
  boolean powerEfficientPixelFormat;
};</pre>
        <section>
          <h2>Dictionary {{MediaTrackSettings}} Members</h2>
          <dl data-link-for="MediaTrackSettings" data-dfn-for=
          "MediaTrackSettings" class="dictionary-members">
            <dt><dfn>powerEfficientPixelFormat</dfn> of type
            {{boolean}}</dt>
            <dd>See <a href=
            "#def-constraint-powerEfficientPixelFormat">
            powerEfficientPixelFormat</a> for details.</dd>
      </section>
      </div>
    </section>
      <h2>Constrainable Properties</h2>
      <p>The constrainable properties in this document are defined below.</p>
      <table class="simple">
        <thead>
          <tr>
            <th>Property Name</th>
            <th>Values</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><dfn id="def-constraint-powerEfficientPixelFormat">
              powerEfficientPixelFormat</dfn></td>
            <td>{{ConstrainBoolean}}</td>
            <td>
              <p>Compressed pixel formats often need to be decoded, for instance
              for display purposes or when being encoded during a video call.
              The user agent SHOULD label compressed pixel formats that incur
              significant power penalty when decoded as power inefficient. The
              labeling is up to the user agent, but decoding MJPEG in software
              is an example of an expensive mode. Pixel formats that have not
              been labeled power inefficient by the user agent are for the
              purpose of this API considered power efficient.</p>
              <p>As a constraint, setting it to true allows filtering out
              inefficient pixel formats and setting it to false allows filtering
              out efficient pixel formats.</p>
              <p>As a setting, this reflects whether or not the current pixel
              format is considered power efficient by the user agent.</p>
            </td>
          </tr>
        </tbody>
      </table>
    </section>
    <section class="informative">
      <h2>Video timestamp concepts</h2>
      <p>
        Video media flowing inside media stream tracks comprises of a sequence of video frames, where
        the frames are sampled from the media at instants spread out over time.
      </p>
      <p>
        Each video frame must have a <dfn class="export">presentation timestamp</dfn>
        which is relative to a source specific origin.
        A source of frames can define how this timestamp is set. A sink of frames
        can define how this timestamp is used.
      </p>
      <p>
        The timestamp is present for sinks to be able to define an absolute presentation timeline of the frames
        relative to a clock reference, for example for playback.
      </p>
      <p>
        Each frame may have an absolute <dfn class="export">capture timestamp</dfn> representing
        the instant the frame capture process began, which is useful for example for
        delay measurements and synchronization.
        A source of frames can define how this timestamp is set, otherwise it is unset. A
        sink of frames can define how this timestamp is used if set.
      </p>
      <p>
        Each frame may have an absolute <dfn class="export">receive timestamp</dfn> representing
        the last received timestamp of packets used to produce this video frame was received in its entirety.
        The timestamp is useful for example for network jitter measurements.
        A source of frames can define how this timestamp is set, otherwise it is unset. A sink of
        frames can define how this timestamp is used if set.
      </p>
      <p>
        Each frame may have a <dfn class="export">RTP timestamp</dfn> representing the packet RTP
        timestamp used to produce this video frame. The timestamp is useful for example for frame
        identification and playback quality measurements. A source of frames can define how the
        timestamp is set, otherwise it is unset. A sink of frames can define how this timestamp is
        used if set.
        The packet RTP timestamp concept is defined in [[?RFC3550]] Section 5.1.
      </p>
      <h3>Timestamp clock relations</h3>
      <p>
        The [=capture timestamp=] and [=receive timestamp=] are using the same clock and offset.
        The [=presentation timestamp=] and [=capture timestamp=] are using the same clock and
        have an offset which can be arbitrarily chosen by the user agent since it isn't
        directly observable by script.
      </p>
      <h3>{{VideoFrameMetadata}}</h3>
        <pre class="idl">
partial dictionary VideoFrameMetadata {
  DOMHighResTimeStamp captureTime;
  DOMHighResTimeStamp receiveTime;
  unsigned long rtpTimestamp;
};</pre>
      <section class="notoc">
        <h5>Members</h5>
        <dl class="dictionary-members" data-link-for="VideoFrameMetadata" data-dfn-for="VideoFrameMetadata">
          <dt><dfn><code>captureTime</code></dfn> of type <span class="idlMemberType">DOMHighResTimeStamp</span></dt>
          <dd>
            <p>The capture timestamp of the frame relative to {{Performance}}.{{Performance/timeOrigin}}. It corresponds to
              the [=capture timestamp=] of {{MediaStreamTrack}} video frames.
            </p>
          </dd>
          <dt><dfn><code>receiveTime</code></dfn> of type <span class="idlMemberType">DOMHighResTimeStamp</span></dt>
          <dd>
            <p>The receive time of the corresponding encoded frame relative to {{Performance}}.{{Performance/timeOrigin}}.
              It corresponds to the [=receive timestamp=] of {{MediaStreamTrack}} video frames.</p>
          </dd>
          <dt><dfn><code>rtpTimestamp</code></dfn> of type <span class="idlMemberType">unsigned long</span></dt>
          <dd>
            <p>The RTP timestamp of the corresponding encoded frame. It corresponds to [=RTP timestamp=] of
            {{MediaStreamTrack}} video frames.</p>
          </dd>
        </dl>
      </section>
      <h3>Algorithms</h3>
      When the <dfn class="abstract-op">Initialize Video Frame Timestamps From Internal MediaStreamTrack Video Frame</dfn>
      algorithm is invoked with |frame| and |offset| as input, run the following steps.
      <ol class=algorithm>
        <li>Set {{VideoFrame/timestamp}} from [=presentation timestamp=] minus |offset|.</li>
        <li>Set {{VideoFrameMetadata/captureTime}} from [=capture timestamp=] if set.</li>
        <li>Set {{VideoFrameMetadata/receiveTime}} from [=receive timestamp=] if set.</li>
        <li>Set {{VideoFrameMetadata/rtpTimestamp}} from [=RTP timestamp=] if set.</li>
      </ol>
      When the <dfn class="abstract-op">Copy Video Frame Timestamps To Internal MediaStreamTrack Video Frame</dfn>
      algorithm runs with |frame| as input, run the following steps.
      <ol class=algorithm>
        <li>Set [=presentation timestamp=] from {{VideoFrame/timestamp}}.</li>
        <li>Set [=capture timestamp=] from {{VideoFrameMetadata/captureTime}} if [=map/exist|present=].</li>
        <li>Set [=receive timestamp=] from {{VideoFrameMetadata/receiveTime}} if [=map/exist|present=].</li>
        <li>Set [=RTP timestamp=] from {{VideoFrameMetadata/rtpTimestamp}} if [=map/exist|present=].</li>
      </ol>
    </section>
    <section>
      <h3>Local video capture timestamps</h3>
      <p>
        The user agent MUST set the [=capture timestamp=] of each video frame that is sourced from
        {{MediaDevices/getUserMedia()}} and {{MediaDevices/getDisplayMedia()}} to its best estimate of the time that
        the frame was captured.
        This value MUST be monotonically increasing.
      </p>
      <div class="note">
        Local capture tracks have a fixed offset between [=presentation timestamp=] and [=capture timestamp=]. The
        user agent may let this be zero with the result that [=presentation timestamp=] is the same as [=capture timestamp=].
      </div>
    </section>

    <section>
    <h2>Exposing MediaStreamTrack source heuristic reactions support</h2>
    <div>
      <p>Some platforms or User Agents may provide built-in support for video effects triggered by user motion heuristics, in particular for camera video streams.
         Web applications may either want to control or at least be aware that these heuristics are active and might trigger these effects at the source level.
         This can for instance allow the web application to update its UI or to turn off these heuristics where having such effects tiggered accidentally might be considered insensitive or inappropriate.
         For that reason, we extend {{MediaStreamTrack}} with the following properties.
      </p>
    </div>
    <div>
      <p>The WebIDL changes are the following:
      <pre class="idl">
partial dictionary MediaTrackSupportedConstraints {
  boolean gestureReactions = true;
};

partial dictionary MediaTrackConstraintSet {
  ConstrainBoolean gestureReactions;
};

partial dictionary MediaTrackSettings {
  boolean gestureReactions;
};

partial dictionary MediaTrackCapabilities {
  sequence&lt;boolean&gt; gestureReactions;
};</pre>
    </div>
  </section>
  <section>
    <h2>Exposing MediaStreamTrack source automatic face framing support</h2>
    <p>Some platforms or User Agents may provide built-in support for automatic
       continuous framing based on the position of human faces within the field
       of view, in particular for camera video streams.
       Web applications may either want to control or at least be aware that
       automatic continuous human face framing is applied at the source level.
       This may for instance allow the web application to update its UI or to
       not apply human face framing on its own.
       For that reason, we extend {{MediaStreamTrack}} with the following
       properties.
    </p>
    <p>The WebIDL changes are the following:</p>
    <pre class="idl"
>partial dictionary MediaTrackSupportedConstraints {
  boolean faceFraming = true;
};

partial dictionary MediaTrackCapabilities {
  sequence&lt;boolean&gt; faceFraming;
};

partial dictionary MediaTrackConstraintSet {
  ConstrainBoolean faceFraming;
};

partial dictionary MediaTrackSettings {
  boolean faceFraming;
};</pre>
    <section>
      <h3>Processing considerations</h3>
      <p>When the "faceFraming" setting is set to <code>true</code> by
         the <a>ApplyConstraints algorithm</a>, the UA will attempt to
         continuously improve framing by cropping to human faces.
      </p>
      <p>When the "faceFraming" setting is set to <code>false</code> by
         the <a>ApplyConstraints algorithm</a>, the UA will not crop to human
         faces.
      </p>
    </section>
    <section>
      <h3>Examples</h3>
      <pre class="example">
&lt;video&gt;&lt;/video&gt;
&lt;script&gt;
// Open camera.
const stream = await navigator.mediaDevices.getUserMedia({video: true});
const [videoTrack] = stream.getVideoTracks();

// Try to improve framing.
const capabilities = videoTrack.getCapabilities();
if ("faceFraming" in capabilities) {
  await videoTrack.applyConstraints({faceFraming: true});
} else {
  // Face framing is not supported by the platform or by the camera.
  // Consider falling back to some other method.
}

// Show to user.
const videoElement = document.querySelector("video");
videoElement.srcObject = stream;
&lt;/script&gt;
      </pre>
    </section>
  </section>
  <section>
    <h2>Exposing MediaStreamTrack source eye gaze correction support</h2>
    <p>Some platforms or User Agents may provide built-in support for human eye
       gaze correction to make the eyes of faces appear to look at the camera,
       in particular for camera video streams.
       This may for instance allow the web application to update its UI or to
       not apply human eye gaze correction on its own.
       For that reason, we extend {{MediaStreamTrack}} with the following
       properties.
    </p>
    <p>The WebIDL changes are the following:</p>
    <pre class="idl"
>partial dictionary MediaTrackSupportedConstraints {
  boolean eyeGazeCorrection = true;
};

partial dictionary MediaTrackCapabilities {
  sequence&lt;boolean&gt; eyeGazeCorrection;
};

partial dictionary MediaTrackConstraintSet {
  ConstrainBoolean eyeGazeCorrection;
};

partial dictionary MediaTrackSettings {
  boolean eyeGazeCorrection;
};</pre>
    <section>
      <h3>Processing considerations</h3>
      <p>When the "eyeGazeCorrection" setting is set to <code>true</code>,
         the User Agent will attempt to correct human eye gaze so that the eyes
         of faces appear to look at the camera.
      </p>
      <p>When the "eyeGazeCorrection" setting is set to <code>false</code>,
         the User Agent will not correct human eye gaze.
      </p>
    </section>
    <section>
      <h3>Examples</h3>
      <pre class="example">
&lt;video&gt;&lt;/video&gt;
&lt;script&gt;
// Open camera.
const stream = await navigator.mediaDevices.getUserMedia({video: true});
const [videoTrack] = stream.getVideoTracks();

// Try to correct eye gaze.
const videoCapabilities = videoTrack.getCapabilities();
if ((videoCapabilities.eyeGazeCorrection || []).includes(true)) {
  await videoTrack.applyConstraints({eyeGazeCorrection: {exact: true}});
} else {
  // Eye gaze correction is not supported by the platform or by the camera.
  // Consider falling back to some other method.
}

// Show to user.
const videoElement = document.querySelector("video");
videoElement.srcObject = stream;
&lt;/script&gt;
      </pre>
    </section>
  </section>
  <section>
    <h2>VoiceIsolation constraint</h2>
    <div>
      <p>Some platforms offer functionality for voice isolation:
      Attempting to remove all parts of an audio track that do not
      correspond to a human voice. Some platforms even attempt to
      remove extraneous voices, leaving the "main voice" as the
        dominant component of the audio. The exact methods used may
      vary between implementations.
      </p>
      <p>
        This constraint permits the platform to turn on that functionality,
        with the desired result being that the "main voice" in the audio
        signal is the dominant component of the audio.
      </p>
      <p>
        This will have large effects on audio that is presented for
        other reasons than to transmit voice (for instance music or
        ambient noises), so needs to be off by default.
      </p>
      <p>
        This constraint is a stronger version of noise cancellation,
        which means that if the "noiseSuppression" constraint is set
        to false and "voiceIsolation" is set to true, the value of
        "noiseCancellation" will be ignored.
      </p>
      <p>
        This constraint has no such relationship with any other
        constraint; in particular it does not affect echoCancellation.
      </p>
    </div>
    <div>
      <p>
        The WebIDL changes are the following:
        <pre class="idl">
partial dictionary MediaTrackSupportedConstraints {
  boolean voiceIsolation = true;
};

partial dictionary MediaTrackConstraintSet {
  ConstrainBoolean voiceIsolation;
};

partial dictionary MediaTrackSettings {
  boolean voiceIsolation;
};

partial dictionary MediaTrackCapabilities {
  sequence&lt;boolean&gt; voiceIsolation;
};
        </pre>
        </p>
      <h4>Processing considerations</h4>
      <p>
        When the "voiceIsolation" setting is set to <code>true</code> by the
        <a>ApplyConstraints algorithm</a>, the UA
        will attempt to remove the components of the audio track that
        do not correspond to a human voice. If a dominant voice can be
        identified, the UA will attempt to enhance that voice.
      </p>
      <p>
        When the "voiceIsolation" constraint setting is set to <code>false</code>
        by the <a>ApplyConstraints algorithm</a>, the UA will process the
        audio according to other settings in its normal fashion.
      </p>
  </section>
  <section>
    <h2>Exposing change of MediaStreamTrack configuration</h2>
    <div>
      <p>The configuration (capabilities and settings) of a {{MediaStreamTrack}} may be changed dynamically
        outside the control of web applications. One example is when a user decides to switch on background blur through
        the operating system. Web applications might want to know that the configuration
        of a particular {{MediaStreamTrack}} has changed. For that purpose, a new event is defined below.
      </p>
    </div>
    <h3>MediaStreamTrack Interface Extensions</h3>
    <div>
    <pre class="idl">
partial interface MediaStreamTrack {
  attribute EventHandler onconfigurationchange;
};</pre>
    <p>The <dfn class=attribute data-dfn-for="MediaStreamTrack">onconfigurationchange</dfn> attribute
      is an [=event handler IDL attribute=] for the `onconfigurationchange`
      [=event handler=], whose [=event handler event type=] is
      <dfn data-dfn-type=event>configurationchange</dfn>.
    </p>
    <p>
      <p>When the [=User Agent=] detects <dfn data-export id="change-track-configuration">a change of configuration</dfn>
       in a <var>track</var>'s underlying source, the [=User Agent=] MUST run the following steps:</p>
      <ol>
        <li><p>If <var>track</var>.{{MediaStreamTrack/muted}} is <code>true</code>, wait for <var>track</var>.{{MediaStreamTrack/muted}}
          to become <code>false</code> or <var>track</var>.{{MediaStreamTrack/readyState}} to be "ended".</p></li>
        <li><p>[=Queue a task=] on [=current settings object=]'s [=environment settings object/responsible event loop=] 
          to perform the following steps:</p>
          <p>
            <div class="note">
              <p>This task will run before any other task that may set <var>track</var>.{{MediaStreamTrack/muted}}
                to <code>true</code>.</p>
            </div>
          </p>
          <ol>
            <li><p>If <var>track</var>.{{MediaStreamTrack/readyState}} is "ended", abort these steps.</p></li>
            <li><p>If <var>track</var>'s capabilities and settings are matching <var>source</var> configuration, abort these steps.
            <li>
              <!-- FIXME: Export capabilities and settings so that we can use them here. -->
              <p>Update <var>track</var>'s capabilities and settings according <var>track</var>'s underlying source.</p>
            </li>
            <li>
              <p>[=Fire an event=] named {{configurationchange}} on <var>track</var>.</p>
            </li>
          </ol>
        </li>
      </ol>
    </p>
    <p>
      <div class="note">
        <p class="fingerprint">These events are potentially triggered simultaneously on documents of different origins.
          [=User Agents=] MAY add fuzzing on the timing of events to avoid cross-origin activity correlation.</p>
      </div>
    </p>
    </div>
    <h3>Example</h3>
    <div>
    <p>This example shows how to monitor external background blur changes.</p>
    <pre class="example">
      const stream = await navigator.mediaDevices.getUserMedia({video: true});
      const [track] = stream.getVideoTracks();
      let {backgroundBlur} = track.getSettings();
      applyBlurInSoftwareInstead(!backgroundBlur);

      track.addEventListener("configurationchange", () => {
        if (backgroundBlur != track.getSettings().backgroundBlur) {
          backgroundBlur = track.getSettings().backgroundBlur;
          applyBlurInSoftwareInstead(!backgroundBlur);
        }
      });
    </pre>
    </div>
  </section>
  <section>
    <h2>Human face segmentation</h2>
    <p>Human face metadata describes the geometrical information of human faces
    in video frames. It can be set by web applications using the standard means
    when creating {{VideoFrameMetadata}} for {{VideoFrame}}s or it can be set by
    a user agent when the media track constraint, defined below, is used to
    enable face detection for the {{MediaStreamTrack}} which provides the
    {{VideoFrame}}s.</p>
    <p>The facial metadata can be used by video encoders to enhance the quality
    of the faces in encoded video streams or for other suitable purposes.</p>
    <section>
      <h3>{{VideoFrameMetadata}}</h3>
      <pre class="idl">
partial dictionary VideoFrameMetadata {
  sequence&lt;Segment&gt; segments;
};</pre>
      <section class="notoc">
        <h4>Members</h4>
        <dl class="dictionary-members" data-link-for="VideoFrameMetadata" data-dfn-for="VideoFrameMetadata">
          <dt><dfn><code>segments</code></dfn> of type <code>sequence&lt;{{Segment}}&gt;</code></dt>
          <dd>
            <p>The set of known geometrical segments in the video frame.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{Segment}}</h3>
      <pre class="idl">
dictionary Segment {
  required SegmentType type;
  required long        id;
  long                 partOf;
  required float       probability;
  Point2D              centerPoint;
  DOMRectInit          boundingBox;
};</pre>
<pre class="idl">
enum SegmentType {
  "human-face",
  "left-eye",
  "right-eye",
  "eye",
  "mouth",
};</pre>
      <section class="notoc">
        <h4>Dictionary {{Segment}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="Segment" data-link-for="HumanFace">
          <dt><dfn><code>type</code></dfn> of type <code>{{SegmentType}}</code></dt>
          <dd>
            <p>The type of segment which the segment refers to.</p>
            <p>It must be one of the following values:</p>
            <dl data-dfn-for="SegmentType" data-link-for="SegmentType">
              <dt><dfn><code>human-face</code></dfn></dt>
              <dd>
                <p>The segment describes a human face.</p>
              </dd>
              <dt><dfn><code>left-eye</code></dfn></dt>
              <dd>
                <p>The segment describes <i>oculus sinister</i>.</p>
              </dd>
              <dt><dfn><code>right-eye</code></dfn></dt>
              <dd>
                <p>The segment describes <i>oculus dexter</i>.</p>
              </dd>
              <dt><dfn><code>eye</code></dfn></dt>
              <dd>
                <p>The segment describes an eye, either left or right.</p>
              </dd>
              <dt><dfn><code>mouth</code></dfn></dt>
              <dd>
                <p>The segment describes a mouth.</p>
              </dd>
            </dl>
          </dd>
          <dt><dfn><code>id</code></dfn> of type {{long}}</dt>
          <dd>
            <p>An identifier of the object described by the segment, unique
	    within a sequence. If the same object can be tracked over multiple
	    frames originating from the same {{MediaStreamTrack}} source or it
	    can be matched to correspond to the same object in
	    {{MediaStreamTrack}}s which are cloned from the same original
	    {{MediaStreamTrack}}, the user agent SHOULD use the same
	    {{Segment/id}} for the segments which describe the object.
            {{Segment/id}} is also used in conjunction with the member
            {{Segment/partOf}}.</p>
            <p>The user agent MUST NOT select the value to assign to
            {{Segment/id}} in such a way that the detected objects could be
            correlated to match between different {{MediaStreamTrack}} sources
            unless the {{MediaStreamTrack}}s are cloned from the same original
            {{MediaStreamTrack}}.
            </p>
          </dd>
          <dt><dfn><code>partOf</code></dfn> of type {{long}}</dt>
          <dd>
            <p>If defined, references another segment which has the member {{Segment/id}} set to the same value.
            The referenced segment corresponds to an object of which the object described by this segment
            is part of.</p>
            <p>If undefined, the object described by this segment is not known to be part of any other object
            described by any segment associated with the {{MediaStreamTrack}}.</p>
          </dd>
          <dt><dfn><code>probability</code></dfn> of type {{float}}</dt>
          <dd>
            <p>If nonzero, this is the estimate of the conditional probability that the segmented object
            actually is of the type indicated by the {{Segment/type}} member on the condition that
            the detection has been made. The value of this member must be always zero or above with a
            maximum of one. The special value of zero indicates that the probability estimate
            is not available.</p>
          </dd>
          <dt><dfn><code>centerPoint</code></dfn> of type {{Point2D}}</dt>
          <dd><p>The coordinates of the approximate center of the object described by this {{Segment}}.
            The object location in the frame can be specified even if it is obscured by other objects in
            front of it or it lies partially or fully outside of the frame.</p>
            <p>The {{Point2D/x}} and {{Point2D/y}} values of the point are interpreted to represent a coordinate in a
            normalized square space. The origin of coordinates {x,y} =
            {0.0, 0.0} represents the upper left corner whereas the {x,y} =
            {1.0, 1.0} represents the lower right corner relative to the
            rendered frame.</p>
          </dd>
          <dt><dfn><code>boundingBox</code></dfn> of type <code>{{DOMRectInit}}</code></dt>
          <dd>
            <p>A bounding box surrounding the object described by this segment. </p>
            <p>The object bounding box in the frame can be specified even if it is
            obscured by other objects in front of it or it lies partially or
            fully outside of the frame.</p>
            <p>See the member {{Segment/centerPoint}} for the definition of the
            coordinate system.
            </p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{MediaTrackSupportedConstraints}} dictionary extensions</h3>
<pre class="idl">
partial dictionary MediaTrackSupportedConstraints {
  boolean humanFaceDetectionMode = true;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{MediaTrackSupportedConstraints}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="MediaTrackSupportedConstraints" data-link-for="MediaTrackSupportedConstraints">
          <dt><dfn>humanFaceDetectionMode</dfn> of type {{boolean}}, defaulting to <code>true</code></dt>
            <dd>See <a href=
            "#def-constraint-humanFaceDetectionMode">
            humanFaceDetectionMode</a> for details.</dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{MediaTrackCapabilities}} dictionary extensions</h3>
      <pre class="idl">
partial dictionary MediaTrackCapabilities {
  sequence&lt;DOMString&gt; humanFaceDetectionMode;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{MediaTrackCapabilities}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="MediaTrackCapabilities" data-link-for="MediaTrackCapabilities">
          <dt><dfn>humanFaceDetectionMode</dfn> of type <code>sequence&lt;{{DOMString}}&gt;</code></dt>
          <dd>
            <p>The sequence of supported face detection modes.
            Each string MUST be one of the members of {{HumanFaceDetectionModeEnum}}.
            See <a href="#def-constraint-humanFaceDetectionMode">
            humanFaceDetectionMode</a> for additional details.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{MediaTrackConstraintSet}} dictionary extensions</h3>
<pre class="idl">
partial dictionary MediaTrackConstraintSet {
  ConstrainDOMString humanFaceDetectionMode;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{MediaTrackConstraintSet}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="MediaTrackConstraintSet" data-link-for="MediaTrackConstraintSet">
          <dt><dfn>humanFaceDetectionMode</dfn> of type {{ConstrainDOMString}}</dt>
          <dd>See <a href=
            "#def-constraint-humanFaceDetectionMode">
            humanFaceDetectionMode</a> for details.</dd>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{MediaTrackSettings}}</h3>
<pre class="idl">
partial dictionary MediaTrackSettings {
  DOMString humanFaceDetectionMode;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{MediaTrackSettings}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="MediaTrackSettings" data-link-for="MediaTrackSettings">
          <dt><dfn>humanFaceDetectionMode</dfn> of type {{DOMString}}</dt>
          <dd>See <a href=
            "#def-constraint-humanFaceDetectionMode">
            humanFaceDetectionMode</a> for details.</dd>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{HumanFaceDetectionModeEnum}}</h3>
<pre class="idl">
enum HumanFaceDetectionModeEnum {
  "none",
  "bounding-box",
  "bounding-box-with-landmark-center-point",
};</pre>
      <section class="notoc">
        <h4>{{HumanFaceDetectionModeEnum}} Enumeration Description</h4>
        <dl data-dfn-for="HumanFaceDetectionModeEnum" data-link-for="HumanFaceDetectionModeEnum">
          <dt><dfn><code>none</code></dfn></dt>
          <dd>
            <p>This {{MediaStreamTrack}} source does not set metadata in
            {{VideoFrameMetadata}} of {{VideoFrame}}s related to human faces or
            human face landmarks, that is, to any {{Segment}} which has
            the {{Segment/type}} set to any of the alternatives listed in
            enumeration {{SegmentType}}.
            As an input, this is interpreted as a command to turn off the
            setting of human face and landmark detection.</p>
          </dd>
          <dt><dfn><code>bounding-box</code></dfn></dt>
          <dd>
            <p>This source sets metadata related to human faces
            (segment type of {{SegmentType/"human-face"}}) including bounding
            box information in the member {{Segment/boundingBox}} of each
            {{Segment}} related to a detected face. The source does not set the
            human face landmark information. As an input, this is interpreted
            as a command to enable the setting of human face detection and to
            find the bounding box of each detected face.</p>
          </dd>
          <dt><dfn><code>bounding-box-with-landmark-center-point</code></dfn></dt>
          <dd>
            <p>With this setting, the source sets a superset of the metadata
            compared to the {{HumanFaceDetectionModeEnum/"bounding-box"}}
            setting. The source sets the same metadata and additionally metadata
            related to human face landmarks (all other {{SegmentType}}s except
            {{SegmentType/"human-face"}}) including center point information
            in the member {{Segment/centerPoint}} of each {{Segment}} related
            to a detected landmark. As an input, this is interpreted as a
            command to enable the setting of human face and face landmark
            detection and to set bounding box related information to face
            segment metadata and to set the center point information of each
            detected face landmark.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h2>Constrainable Properties</h2>
      <p>The constrainable properties in this section are defined below.</p>
      <table class="simple">
        <thead>
          <tr>
            <th>Property Name</th>
            <th>Values</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><dfn id="def-constraint-humanFaceDetectionMode">
              humanFaceDetectionMode</dfn></td>
            <td>{{ConstrainDOMString}}</td>
            <td>
              <p>This string (or each string, when a list) should be one of the
              members of {{HumanFaceDetectionModeEnum}}.</p>
              <p>As a {{MediaStreamTrack}} constraint, its value allows choosing
              whether face and face landmark detection is preferred.</p>
              <p>As a setting, this reflects which face geometrical
              properties the user agent detects and sets in the metadata of the
              {{VideoFrame}}s obtained from the track.</p>
            </td>
          </tr>
        </tbody>
      </table>
    </section>
    <section>
      <h3>Examples</h3>
      <pre class="example">
// main.js:
// Open camera with face detection enabled
const stream = await navigator.mediaDevices.getUserMedia({
  video: { humanFaceDetectionMode: 'bounding-box' }
});
const [videoTrack] = stream.getVideoTracks();
if (videoTrack.getSettings().humanFaceDetectionMode != 'bounding-box') {
  throw('Face bounding box detection is not supported');
}

// Use a video worker and show to user.
const videoElement = document.querySelector('video');
const videoWorker = new Worker('video-worker.js');
videoWorker.postMessage({track: videoTrack}, [videoTrack]);
const {data} = await new Promise(r => videoWorker.onmessage);
videoElement.srcObject = new MediaStream([data.videoTrack]);

// video-worker.js:
self.onmessage = async ({data: {track}}) => {
  const generator = new VideoTrackGenerator();
  parent.postMessage({videoTrack: generator.track}, [generator.track]);
  const {readable} = new MediaStreamTrackProcessor({track});
  const transformer = new TransformStream({
    async transform(frame, controller) {
      for (const segment of frame.metadata().segments || []) {
        if (segment.type === 'human-face') {
          // the metadata is coming directly from the video track with
          // bounding-box face detection enabled
          console.log(
            `Face @ (${segment.boundingBox.x}, ${segment.boundingBox.y}), size ` +
            `${segment.boundingBox.width}x${segment.boundingBox.height}`);
        }
      }
      controller.enqueue(frame);
    }
  });
  await readable.pipeThrough(transformer).pipeTo(generator.writable);
};
      </pre>
    </section>
  </section>
  <section>
    <h2>Background Blur Effect Status</h2>
    <p>The {{VideoFrameMetadata}} interface exposes the blur state as a property,
      which allows apps to know the state for every frame. This can be helpful for
      scenarios where the app wishes to help protect user privacy by never sending an
      un-blurred frame off of the user's system.</p>
    <section>
      <h3>{{VideoFrameMetadata}}</h3>
      <pre class="idl">
partial dictionary VideoFrameMetadata {
  BackgroundBlur backgroundBlur;
};</pre>
      <section class="notoc">
        <h4>Members</h4>
        <dl class="dictionary-members" data-link-for="VideoFrameMetadata" data-dfn-for="VideoFrameMetadata">
          <dt><dfn><code>backgroundBlur</code></dfn> of type <code>{{BackgroundBlur}}</code></dt>
          <dd>
            <p>The state of the background blur effect for the current frame, if
              [=map/exist|present=]. Absence might indicate that the frame is not
              from a camera, or the user agent might not support reporting blur state.
            </p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{BackgroundBlur}}</h3>
      <pre class="idl">
dictionary BackgroundBlur: MediaEffectInfo {};</pre>
    </section>
    <section>
      <h3>{{MediaEffectInfo}}</h3>
      <pre class="idl">
dictionary MediaEffectInfo {
  required boolean enabled;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{MediaEffectInfo}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="MediaEffectInfo" data-link-for="MediaEffectInfo">
          <dt><dfn><code>enabled</code></dfn> of type {{boolean}}</dt>
          <dd>
            <p>True if the effect is enabled, false otherwise. This isn't a strong guarantee, as user
              agents likely can't detect all possible video manipulation software.
            </p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>Example</h3>
      <pre class="example">
const stream = await navigator.mediaDevices.getUserMedia({ video: true });
const track = stream.getVideoTracks()[0];
const worker = new Worker(`data:text/javascript,(${work.toString()})()`);
worker.postMessage({track}, [track]);
const {data} = await new Promise(r => worker.onmessage = r);
video.srcObject = new MediaStream([data.track]);

function work() {
  onmessage = async ({data: {track}}) => {
    const trackGenerator = new VideoTrackGenerator({ kind: "video" });
    self.postMessage({track: trackGenerator.track}, [trackGenerator.track]);
    const trackProcessor = new MediaStreamTrackProcessor({ track });

    const transformer = new TransformStream({
      async transform(videoFrame, controller) {
        if ("backgroundBlur" in videoFrame.metadata()) {
          console.log('backgroundBlur.enabled:', videoFrame.metadata().backgroundBlur.enabled);
        } else {
          console.log('backgroundBlur unsupported');
        }

        controller.enqueue(videoFrame);
      },
    });

    trackProcessor.readable
      .pipeThrough(transformer)
      .pipeTo(trackGenerator.writable);
  };
}
      </pre>
    </section>
  </section>
  <section>
    <h2>MediaStream in workers</h2>
    <div>
      <p>{{MediaStreamTrack}}'s objects are exposed to workers, so can do {{MediaStream}}'s objects.
    <div>
      <p>The WebIDL changes are the following:
      <pre class="idl"
>[Exposed=(Window,Worker)]
partial interface MediaStream {
};</pre>
    </div>
  </section>
  <section>
    <h2>Background segmentation mask</h2>
    <p>Some platforms or User Agents may provide built-in support for background segmentation of video frames, in particular for camera video streams.
       Web applications may want to control whether background segmentation is computed at the source level and to get access to the computed segmentation masks.
       This allows the web application for instance
       to do custom framing or background blurring or replacement
       while leveraging on platform computed background segmentation.
       This allows the web application
       to access the original unmodified frame and
       to fine tune frame modifications based on its likings.
       For that reason, we extend {{MediaStreamTrack}} with the following properties and {{VideoFrameMetadata}} with the following attributes.
    </p>
    <pre class="idl">
partial dictionary MediaTrackSupportedConstraints {
  boolean backgroundSegmentationMask = true;
};

partial dictionary MediaTrackConstraintSet {
  ConstrainBoolean backgroundSegmentationMask;
};

partial dictionary MediaTrackSettings {
  boolean backgroundSegmentationMask;
};

partial dictionary MediaTrackCapabilities {
  sequence&lt;boolean&gt; backgroundSegmentationMask;
};</pre>
    <section>
      <h3>{{VideoFrameMetadata}}</h3>
      <pre class="idl">
partial dictionary VideoFrameMetadata {
  ImageBitmap backgroundSegmentationMask;
};</pre>
      <section class="notoc">
        <h4>Members</h4>
        <dl class="dictionary-members" data-link-for="VideoFrameMetadata" data-dfn-for="VideoFrameMetadata">
          <dt><dfn><code>backgroundSegmentationMask</code></dfn> of type <code>{{ImageBitmap}}</code></dt>
          <dd>
            <p>A background segmentation mask with
               white denoting certainly foreground,
               black denoting certainly background and
               grey denoting uncertainty or ambiguity with
               light shades of grey denoting likely foreground and
               dark shades of grey denoting likely background.
               Absence might indicate
               that the frame is not from a camera, or
               that the user agent does not support or
               was not able to do background segmentation.
            </p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>Example</h3>
      <pre class="example">
// Open camera.
const stream = await navigator.mediaDevices.getUserMedia({video: true});
const [videoTrack] = stream.getVideoTracks();

// Try to enable background segmentation mask.
const videoCapabilities = videoTrack.getCapabilities();
if ((videoCapabilities.backgroundSegmentationMask || []).includes(true)) {
  await videoTrack.applyConstraints({backgroundSegmentationMask: {exact: true}});
} else {
  // Background segmentation mask is not supported by the platform or
  // by the camera. Consider falling back to some other method.
}

const canvasContext = document.querySelector('canvas').getContext('2d');
const videoProcessor = new MediaStreamTrackProcessor({track: videoTrack});
const videoProcessorReader = videoProcessor.readable.getReader();

for (;;) {
  const {done, value: videoFrame} = await videoProcessorReader.read();
  if (done)
    break;
  const {backgroundSegmentationMask} = videoFrame.metadata();
  if (backgroundSegmentationMask) {
    // Draw the video frame.
    canvasContext.globalCompositeOperation = 'copy';
    context.drawImage(videoFrame, 0, 0);
    // Draw (or multiply with) the mask.
    // The result is the foreground on black.
    context.globalCompositeOperation = 'multiply';
    canvasContext.drawImage(backgroundSegmentationMask, 0, 0);
  }
  else {
    // Everything is background. Fill with black.
    canvasContext.globalCompositeOperation = 'copy';
    canvasContext.fillStyle = 'black';
    canvasContext.fillRect(
      0,
      0,
      canvasContext.canvas.width,
      canvasContext.canvas.height);
  }
}
      </pre>
    </section>
  </section>
</body>
</html>
