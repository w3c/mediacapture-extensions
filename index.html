<!doctype html>
<html lang="en-us">
<head>
  <title>Media Capture and Streams Extensions</title>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
  <script src="https://www.w3.org/Tools/respec/respec-w3c" class="remove"></script>
  <script class='remove'>
  "use strict";
  // See https://github.com/w3c/respec/wiki/ for how to configure ReSpec
  var respecConfig = {
    group: "webrtc",
    xref: ["html", "infra", "permissions", "dom", "image-capture", "mediacapture-streams", "webaudio", "webcodecs", "webidl"],
    edDraftURI: "https://w3c.github.io/mediacapture-extensions/",
    editors:  [
      {name: "Jan-Ivar Bruaroey", company: "Mozilla Corporation", w3cid: 79152},
    ],
    shortName: "mediacapture-extensions",
    specStatus: "ED",
    subjectPrefix: "[mediacapture-extensions]",
    github: "https://github.com/w3c/mediacapture-extensions",
  };
  </script>
</head>

<body>
  <section id="abstract">
    <p>This document defines a set of ECMAScript APIs in WebIDL to extend the [[mediacapture-streams]] specification.</p>
  </section>
  <section id="sotd">
    <p>This is an unofficial proposal.</p>
  </section>
  <section id="introduction">
    <h2>Introduction</h2>
    <p>This document contains proposed extensions and modifications to the
    [[mediacapture-streams]] specification.</p>
    <p>New features and modifications to existing features proposed here may be
    considered for addition into the main specification post Recommendation.
    Deciding factors will include maturity of the extension or modification,
    consensus on adding it, and implementation experience.</p>
    <p>A concrete long-term goal is reducing the fingerprinting surface of
    {{MediaDevices/enumerateDevices()}} by deprecating exposure of the device
    {{MediaDeviceInfo/label}} in its results. This requires relieving
    applications of the burden of building user interfaces to select cameras and
    microphones in-content, by offering this in user agents as part of
    {{MediaDevices/getUserMedia()}} instead.</p>
    <p>Miscellaneous other smaller features are under consideration as well,
    such as constraints to control multi-channel audio beyond stereo.</p>
  </section>
  <section>
    <h2>Terminology</h2>
    <p>
      This document uses the definitions {{MediaDevices}}, {{MediaStreamTrack}},
      {{MediaStreamConstraints}}, {{ConstrainablePattern}},
      {{MediaTrackSupportedConstraints}}, {{MediaTrackCapabilities}},
      {{MediaTrackConstraintSet}}, {{MediaTrackSettings}} and
      {{ConstrainBoolean}} from [[!mediacapture-streams]].
      <p>The terms [=permission state=], [=request permission to use=], and
      <a data-cite="permissions">prompt the user to choose</a> are defined in
      [[!permissions]].</p>    </p>
  </section>
  <section id="conformance">
  </section>
  <section id="camera and microphone picker">
    <h2>In-browser camera and microphone picker</h2>
    <p>The existing {{MediaDevices/enumerateDevices()}} function exposes camera
    and microphone {{MediaDeviceInfo/label}}s to let applications build
    in-content user interfaces for camera and microphone selection. Applications
    have had to do this because {{MediaDevices/getUserMedia()}} did not offer a
    web compatible in-agent device picker. This specification aims to rectify
    that.</p>
    <p>Due to the significant fingerprinting vector caused by device
    {{MediaDeviceInfo/label}}s, and the well-established nature of the existing
    APIs, the scope of this particular effort is limited to removing
    {{MediaDeviceInfo/label}}, leaving the overall constraints-based model
    intact. This helps ensure a migration path more viable than to a
    less-powerful API.</p>
    <p>This specification augments the existing {{MediaDevices/getUserMedia()}}
    function instead of introducing a new less-powerful API to compete with it,
    for that reason as well.</p>
    <section id="new getusermedia semantics">
      <h3>getUserMedia "user-chooses" semantics</h3>
      <p>This specification introduces
      slightly altered semantics to the {{MediaDevices/getUserMedia()}}
      function called <code>"user-chooses"</code> that guarantee a picker will
      be shown to the user in cases where the user agent would otherwise choose
      for the user (that is: when application constraints do not narrow down
      the choices to a single device). This is orthoginal to permission, and
      offers a better and more consistent user experience across applications
      and user agents.
      </p>
      <p>Unfortunately, since the <code>"user-chooses"</code> semantics may
      produce user agent prompts at different times and in different situations
      compared to the old semantics, they are somewhat incompatible with
      expectations in some existing web applications that tend to call
      {{MediaDevices/getUserMedia()}} repeatedly and lazily instead of using
      e.g. <code>stream.clone()</code>.</p>
    </section>
    <section id="web compatibility">
      <h3>Web compatibility and migration</h3>
      <p>User agents are encouraged to provide the new semantics as opt-in
      initially for web compatibility. User agents MUST deprecate (remove)
      {{MediaDeviceInfo/label}} from {{MediaDeviceInfo}} over time, though specific migration strategies
      are left to user agents. User agents SHOULD migrate to offering the new
      semantics by default (opt-out) over time.</p>
      <p>Since the constraints-model remains intact, web compatibility problems
      are expected to be limited to:</p>
      <ul>
        <li>
          Sites that never migrated show e.g. "Camera 1", "Camera 2" etc.
          instead of descriptive device labels
        </li>
        <li>
          Sites with no device management strategy provoke a picker in the
          user agent every visit for users with more than a singular choice
          of camera or microphone (a feature of sorts)
        </li>
      </ul>
    </section>
    <section id="mediadevices-interface">
      <h3>MediaDevices Interface Extensions</h3>
      <div>
        <pre class="idl"
>partial interface MediaDevices {
  readonly attribute GetUserMediaSemantics defaultSemantics;
};</pre>
      </div>
      <section>
        <h2>Attributes</h2>
        <dl data-link-for="MediaDevices" data-dfn-for="MediaDevices"
        class="attributes">
          <dt id="def-mediadevices-defaultSemantics"><dfn><code>defaultSemantics</code></dfn>
          of type <span class="idlAttrType"><a>GetUserMediaSemantics</a></span>, readonly</dt>
          <dd>
            <p>The default semantics of {{MediaDevices/getUserMedia()}} in this
            user agent.</p>
            <p>User agents SHOULD default to <code>"browser-chooses"</code>
            for backwards compatibility, until a transition plan has been
            enacted where a majority of user agents collectively switch their
            defaults to <code>"user-chooses"</code> for improved user privacy,
            and usage metrics suggest this transition is feasible without
            major breakage.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section id="mediastreamconstraints-dictionary-extensions">
      <h3>MediaStreamConstraints dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaStreamConstraints {
  GetUserMediaSemantics semantics;
};</pre>
        <section>
          <h2>Dictionary {{MediaStreamConstraints}} Members</h2>
          <dl data-link-for="MediaStreamConstraints" data-dfn-for=
          "MediaStreamConstraints" class="dictionary-members">
            <dt><dfn><code>semantics</code></dfn> of type <span class=
            "idlMemberType">{{GetUserMediaSemantics}}</span></dt>
            <dd>
              <p>In cases where the specified constraints do not narrow
              multiple choices between devices down to one per kind, specifies
              how the final determination of which devices to pick from the
              remaining choices MUST be made. If not specified, then the
              <a data-link-for="MediaDevices">defaultSemantics</a> are used.
              </p>
            </dd>
          </dl>
        </section>
      </div>
    </section>
    <section id="getusermediasemantics-enum">
      <h3>GetUserMediaSemantics enum</h3>
      <div>
        <pre class="idl"
>enum GetUserMediaSemantics {
  "browser-chooses",
  "user-chooses"
};</pre>
        <table data-link-for="GetUserMediaSemantics" data-dfn-for=
        "GetUserMediaSemantics" class="simple">
          <tbody>
            <tr>
              <th colspan="2"><dfn>GetUserMediaSemantics</dfn> Enumeration
              description</th>
            </tr>
            <tr>
              <td><dfn><code id=
              "idl-def-GetUserMediaSemantics.browser-chooses">browser-chooses</code></dfn></td>
              <td>
                <p>When application-specified constraints do not narrow multiple
                choices between devices down to one per kind, the user agent is
                allowed to make the final determination between the remaining
                choices.
                </p>
              </td>
            </tr>
            <tr>
              <td><dfn><code id=
              "idl-def-GetUserMediaSemantics.user-chooses">user-chooses</code></dfn></td>
              <td>
                <p>When application-specified constraints do not narrow
                multiple choices between devices down to one per kind, the user
                agent MUST
                <a href="prompt-the-user-to-choose">prompt the user to choose</a>
                between the remaining choices, even if the application already
                has permission to some or all of them.</p>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>
    <section>
      <h3>Algorithms</h3>
      <p>When the {{MediaDevices/getUserMedia()}} method is invoked, run the
      following steps before invoking the {{MediaDevices/getUserMedia()}}
      algorithm:</p>
      <ol>
          <li>
            <p>Let <var>mediaDevices</var> be the object on which this method was
            invoked.</p>
          </li>
          <li>
            <p>Let <var>constraints</var> be the method's first argument.</p>
          </li>
          <li>
            <p>Let <var>semanticsPresent</var> be <code>true</code> if
            <var>constraints</var><code>.semantics</code> [= map/exists =],
            otherwise <code>false</code>.</p>
          </li>
          <li>
            <p>Let <var>semantics</var> be
            <var>constraints</var><code>.semantics</code>
            if <a href="https://heycam.github.io/webidl/#dfn-present">present</a>,
            or the value of <var>mediaDevices</var><code>.<a data-link-for="MediaDevices">defaultSemantics</a></code>
            otherwise.</p>
          </li>
          <li>
            <p>Replace step 6.5.1. of the {{MediaDevices/getUserMedia()}}
            algorithm in its entirety with the following two steps:</p>
            <ol>
              <li>
                <p>Let <var>descriptor</var> be a {{PermissionDescriptor}}
                with its {{PermissionDescriptor/name}} member set to the permission name
                associated with <var>kind</var> (e.g. {{PermissionName/"camera"}} for
                <code>"video"</code>, {{PermissionName/"microphone"}} for <code>"audio"</code>), and,
                optionally, consider its {{DevicePermissionDescriptor/deviceId}} member set to any appropriate
                device's <var>deviceId</var>.</p>
              </li>
              <li>
                <p>If the number of unique devices sourcing tracks of
                media type <var>kind</var> in <var>candidateSet</var>
                is greater than <code>1</code> and
                <var>semantics</var> is <code>"user-chooses"</code>,
                then <a>prompt the user to choose</a> a device with
                <var>descriptor</var>, resulting in provided media.
                Otherwise, <a>request permission to use</a> a
                device with <var>descriptor</var>, while considering
                all devices being attached to a live and
                <a>same-permission</a> MediaStreamTrack in the current
                [=browsing
                context=] to mean having permission status {{PermissionState/"granted"}},
                resulting in provided media.</p>
                <p><dfn>Same-permission</dfn> in this context means a
                {{MediaStreamTrack}} that required the same level of
                permission to obtain as what is being requested.</p>
                <p>When asking the userâ€™s permission, the user agent
                MUST disclose whether permission will be granted only to
                the device chosen, or to all devices of that
                <var>kind</var>.</p>
                <p>Let <var>track</var> be the provided media, which
                MUST be precisely one track of type <var>kind</var> from
                <var>finalSet</var>. If <var>semantics</var> is
                <code>"browser-chooses"</code> then the decision of
                which track to choose from <var>finalSet</var> is up
                to the User Agent, which MAY use the value of the computed
                "fitness distance" from the <a href=
                "https://www.w3.org/TR/mediacapture-streams/#dfn-selectsettings">
                SelectSettings</a>
                algorithm, the value of <var>semanticsPresent</var>,
                or any other internally-available information about
                the devices, as inputs to its decision.
                If <var>semantics</var> is <code>"user-chooses"</code>,
                and the application has not narrowed down the choices
                to one, then the user agent MUST ask the user to make
                the final selection.</p>
                <p>Once selected, the source of the
                {{MediaStreamTrack}} MUST NOT change.</p>
                <p>User Agents are encouraged to default to or present
                a default choice based primarily on fitness distance,
                and secondarily on the user's primary or system default
                device for <var>kind</var> (when possible). User Agents
                MAY allow users to use any media source, including
                pre-recorded media files.</p>
              </li>
            </ol>
          </li>
      </ol>
    </section>
    <section>
      <h3>Examples</h3>
      <div>
        <p>This example shows a setup with a start button and a camera selector
        using the new semantics (microphone is not shown for brievity but is
        equivalent).</p>
        <pre class="example">
&lt;button id="start"&gt;Start&lt;/button&gt;
&lt;button id="chosenCamera" disabled&gt;Camera: none&lt;/button&gt;
&lt;script&gt;

let cameraTrack = null;

start.onclick = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: {deviceId: localStorage.cameraId}
    });
    setCameraTrack(stream.getVideoTracks()[0]);
  } catch (err) {
    console.error(err);
  }
}

chosenCamera.onclick = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: true,
      semantics: "user-chooses"
    });
    setCameraTrack(stream.getVideoTracks()[0]);
  } catch (err) {
    console.error(err);
  }
}

function setCameraTrack(track) {
  cameraTrack = track;
  const {deviceId, label} = track.getSettings();
  localStorage.cameraId = deviceId;
  chosenCamera.innerText = `Camera: ${label}`;
  chosenCamera.disabled = false;
}
&lt;/script&gt;
        </pre>
      </div>
    </section>
  </section>
  <section>
    <h2>Transferable MediaStreamTrack</h2>
    <div>
      <p>A {{MediaStreamTrack}} is a <a data-cite="!HTML/#transferable-objects">transferable object</a>.
      This allows manipulating real-time media outside the context it was requested or created in,
      for instance in workers or third-party iframes.</p>
      <p>To preserve the existing privacy and security infrastructure, in particular for capture tracks,
      the track source lifetime management remains tied to the context that created it.
      The transfer algorithm MUST ensure the following behaviors:</p>
      <p>
        <ol>
          <li><p>The context named <var>originalContext</var> that created a track named <var>originalTrack</var> remains in control
          of the <var>originalTrack</var> source, named <var>trackSource</var>, even when <var>originalTrack</var> is transferred into <var>transferredTrack</var>.</p>
          </li>
          <li>
            <p>In particular, <var>originalContext</var> remains the proxy to privacy indicators of <var>trackSource</var>.
            <var>transferredTrack</var> or any of its clones are considered as tracks using <var>trackSource</var>
            as if they were tracks created in and controlled by <var>originalContext</var>.</p>
          </li>
          <li><p>When <var>originalContext</var> goes away, <var>trackSource</var> gets ended, thus <var>transferredTrack</var> gets ended.</p></li>
          <li><p>When <var>originalContext</var> would have muted/unmuted <var>originalTrack</var>, <var>transferredTrack</var> gets muted/unmuted.</p></li>
          <li><p>If <var>transferredTrack</var> is cloned in <var>transferredTrackClone</var>, <var>transferredTrackClone</var> is tied to <var>trackSource</var>.
          It is not tied to <var>originalTrack</var> in any way.</p></li>
          <li><p>If <var>transferredTrack</var> is transferred into <var>transferredAgainTrack</var>, <var>transferredAgainTrack</var> is tied to <var>trackSource</var>.
          It is not tied to <var>transferredTrack</var> or <var>originalTrack</var> in any way.</p></li>
        </ol>
      </p>
    </div>
    <div>
      <p>The WebIDL changes are the following:
      <pre class="idl"
>[Exposed=(Window,Worker), Transferable]
partial interface MediaStreamTrack {
};</pre>
    </div>
    <div>
      <p>At creation of a {{MediaStreamTrack}} object, called <var>track</var>, run the following steps:</p>
      <ol>
        <li><p>Initialize <var>track</var>.`[[IsDetached]]` to <code>false</code>.</p></li>
      </ol>
    </div>
    <div>
      <p>The {{MediaStreamTrack}} <a data-cite="!HTML/#transfer-steps">transfer steps</a>, given <var>value</var> and <var>dataHolder</var>, are:</p>
      <ol>
        <li><p>If <var>value</var>.`[[IsDetached]]` is <code>true</code>, throw a "DataCloneError" DOMException.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[id]]` to <var>value</var>.{{MediaStreamTrack/id}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[kind]]` to <var>value</var>.{{MediaStreamTrack/kind}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[label]]` to <var>value</var>.{{MediaStreamTrack/label}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[readyState]]` to <var>value</var>.{{MediaStreamTrack/readyState}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[enabled]]` to <var>value</var>.{{MediaStreamTrack/enabled}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[muted]]` to <var>value</var>.{{MediaStreamTrack/muted}}.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[source]]` to <var>value</var> underlying source.</p></li>
        <li><p>Set <var>dataHolder</var>.`[[constraints]]` to <var>value</var> active constraints.</p></li>
        <li><p>Set <var>value</var>.`[[IsDetached]]` to <code>true</code>.</p></li>
        <li><p>Set <var>value</var>.{{MediaStreamTrack/[[ReadyState]]}} to <a data-cite="!mediacapture-streams/#track-ended">"ended"</a> (without stopping the underlying source or firing an `ended` event).</p></li>
      </ol>
    </div>
    <div><p>{{MediaStreamTrack}} <a data-cite="!HTML/#transfer-receiving-steps">transfer-receiving steps</a>, given <var>dataHolder</var> and <var>track</var>, are:</p>
      <ol>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/id}} to <var>dataHolder</var>.`[[id]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/kind}} to <var>dataHolder</var>.`[[kind]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/label}} to <var>dataHolder</var>.`[[label]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/readyState}} to <var>dataHolder</var>.`[[readyState]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/enabled}} to <var>dataHolder</var>.`[[enabled]]`.</p></li>
        <li><p>Initialize <var>track</var>.{{MediaStreamTrack/muted}} to <var>dataHolder</var>.`[[muted]]`.</p></li>
        <li><p>[=MediaStreamTrack/Initialize the underlying source=] of <var>track</var> to <var>dataHolder</var>.`[[source]]`.</p></li>
        <li><p>Set <var>track</var>'s constraints to <var>dataHolder</var>.`[[constraints]]`.</p></li>
      </ol>
    </div>
    <div>
      <p>The underlying source is supposed to be kept alive between the transfer and transfer-receiving steps, or as long as the data holder is alive.
      In a sense, between these steps, the data holder is attached to the underlying source as if it was a track.</p>
    </div>
  </section>
  <section id="the powerEfficientPixelFormat constraint">
    <h2>The powerEfficientPixelFormat constraint</h2>
    <section id="mediatracksupportedconstraints-dictionary-extensions">
      <h3>MediaTrackSupportedConstraints dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaTrackSupportedConstraints {
  boolean powerEfficientPixelFormat = true;
};</pre>
        <section>
          <h2>Dictionary {{MediaTrackSupportedConstraints}} Members</h2>
          <dl data-link-for="MediaTrackSupportedConstraints" data-dfn-for=
          "MediaTrackSupportedConstraints" class="dictionary-members">
            <dt><dfn>powerEfficientPixelFormat</dfn> of type
            {{boolean}}, defaulting to <code>true</code></dt>
            <dd>See <a href=
            "#def-constraint-powerEfficientPixelFormat">
            powerEfficientPixelFormat</a> for details.</dd>
          </dl>
        </section>
      </div>
    </section>
    <section id="mediatrackcapabilities-dictionary-extensions">
      <h3>MediaTrackCapabilities dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaTrackCapabilities {
  sequence&lt;boolean&gt; powerEfficientPixelFormat;
};</pre>
        <section>
          <h2>Dictionary {{MediaTrackCapabilities}} Members</h2>
          <dl data-link-for="MediaTrackCapabilities" data-dfn-for=
          "MediaTrackCapabilities" class="dictionary-members">
            <dt><dfn>powerEfficientPixelFormat</dfn> of type
            <code>sequence&lt;{{boolean}}&gt;</code></dt>
            <dd>
              <p>If the source only has power efficient pixel formats, a single
              <code>true</code> is reported. If the source only has power
              inefficient pixel formats, a single <code>false</code> is
              reported. If the script can control the feature, the source
              reports a list with both <code>true</code> and <code>false</code>
              as possible values. See <a href=
              "#def-constraint-powerEfficientPixelFormat">
              powerEfficientPixelFormat</a> for additional
              details.</p>
            </dd>
          </dl>
        </section>
      </div>
    </section>
    <section id="mediatracksettings-dictionary-extensions">
      <h3>MediaTrackSettings dictionary extensions</h3>
      <div>
        <pre class="idl"
>partial dictionary MediaTrackSettings {
  boolean powerEfficientPixelFormat;
};</pre>
        <section>
          <h2>Dictionary {{MediaTrackSettings}} Members</h2>
          <dl data-link-for="MediaTrackSettings" data-dfn-for=
          "MediaTrackSettings" class="dictionary-members">
            <dt><dfn>powerEfficientPixelFormat</dfn> of type
            {{boolean}}</dt>
            <dd>See <a href=
            "#def-constraint-powerEfficientPixelFormat">
            powerEfficientPixelFormat</a> for details.</dd>
      </section>
      </div>
    </section>
      <h2>Constrainable Properties</h2>
      <p>The constrainable properties in this document are defined below.</p>
      <table class="simple">
        <thead>
          <tr>
            <th>Property Name</th>
            <th>Values</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><dfn id="def-constraint-powerEfficientPixelFormat">
              powerEfficientPixelFormat</dfn></td>
            <td>{{ConstrainBoolean}}</td>
            <td>
              <p>Compressed pixel formats often need to be decoded, for instance
              for display purposes or when being encoded during a video call.
              The user agent SHOULD label compressed pixel formats that incur
              significant power penalty when decoded as power inefficient. The
              labeling is up to the user agent, but decoding MJPEG in software
              is an example of an expensive mode. Pixel formats that have not
              been labeled power inefficient by the user agent are for the
              purpose of this API considered power efficient.</p>
              <p>As a constraint, setting it to true allows filtering out
              inefficient pixel formats and setting it to false allows filtering
              out efficient pixel formats.</p>
              <p>As a setting, this reflects whether or not the current pixel
              format is considered power efficient by the user agent.</p>
            </td>
          </tr>
        </tbody>
      </table>
    </section>
  <section>
    <h2>Exposing MediaStreamTrack source background blur support</h2>
    <div>
      <p>Some platforms or User Agents may provide built-in support for background blurring of video frames, in particular for camera video streams.
         Web applications may either want to control or at least be aware that background blur is applied at the source level.
         This may for instance allow the web application to update its UI or to not apply background blur on its own.
         For that reason, we extend {{MediaStreamTrack}} with the following properties.
      </p>
    </div>
    <div>
      <p>The WebIDL changes are the following:
      <pre class="idl">
partial dictionary MediaTrackSupportedConstraints {
  boolean backgroundBlur = true;
};

partial dictionary MediaTrackConstraintSet {
  ConstrainBoolean backgroundBlur;
};

partial dictionary MediaTrackSettings {
  boolean backgroundBlur;
};

partial dictionary MediaTrackCapabilities {
  sequence&lt;boolean&gt; backgroundBlur;
};</pre>
    </div>
  </section>
  <section>
    <h2>Exposing change of MediaStreamTrack configuration</h2>
    <div>
      <p>The configuration (capabilities, constraints or settings) of a {{MediaStreamTrack}} may be changed dynamically
        outside the control of web applications. One example is when a user decides to switch on background blur through
        the operating system. Web applications might want to know that the configuration
        of a particular {{MediaStreamTrack}} has changed. For that purpose, a new event is defined below.
     </p>
      <pre class="idl">
partial interface MediaStreamTrack {
  attribute EventHandler onconfigurationchange;
};</pre>
    <p>
      <p>When the [=User Agent=] detects <dfn data-export id="change-track-configuration">a change of configuration</dfn>
       in a <var>track</var>'s underlying source, the [=User Agent=] MUST run the following steps:</p>
      <ol>
        <li><p>If <var>track</var>.{{MediaStreamTrack/muted}} is <code>true</code>, wait for <var>track</var>.{{MediaStreamTrack/muted}}
          to become <code>false</code> or <var>track</var>.{{MediaStreamTrack/readyState}} to be "ended".</p></li>
        <li><p>If <var>track</var>.{{MediaStreamTrack/readyState}} is "ended", abort these steps.</p></li>
        <li><p>If <var>track</var>'s capabilities, constraints and settings are matching <var>source</var> configuration, abort these steps.
        <li>
          <!-- FIXME: Export capabilities, constraints and settings so that we can use them here. -->
          <p>Update <var>track</var>'s capabilities, constraints and settings according <var>track</var>'s underlying source.</p>
        </li>
        <li>
          <p>[=Fire an event=] named <var>configurationchange</var> on <var>track</var>.</p>
        </li>
      </ol>
    </p>
    <p>
      <div class="note">
        <p class="fingerprint">These events are potentially triggered simultaneously on documents of different origins.
          [=User Agents=] MAY add fuzzing on the timing of events to avoid cross-origin activity correlation.</p>
      </div>
    </p>
    </div>
  </section>
  <section>
    <h2>Face detection</h2>
    <section>
      <h3>{{VideoFrame}}</h3>
      <pre class="idl"
>partial interface VideoFrame {
  readonly attribute FrozenArray&lt;DetectedFace&gt;? detectedFaces;
};</pre>
      <section class="notoc">
        <h4>Attributes</h4>
        <dl class="attributes" data-link-for="VideoFrame" data-dfn-for="VideoFrame">
          <dt><dfn><code>detectedFaces</code></dfn> of type <span class="idlAttrType">{{FrozenArray}}&lt;{{DetectedFace}}&gt;</span>, readonly, nullable</dt>
          <dd>
            <p>A series of detected faces in this video frame.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{DetectedFace}}</h3>
      <pre class="idl"
>dictionary DetectedFace {
  required long                     id;
  required float                    probability;
  FrozenArray&lt;Point2D&gt;              contour;
  FrozenArray&lt;Point2D&gt;              mesh;
  FrozenArray&lt;DetectedFaceLandmark&gt; landmarks;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{DetectedFace}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="DetectedFace" data-link-for="DetectedFace">
          <dt><dfn><code>id</code></dfn> of type <span class="idlMemberType">{{long}}</span></dt>
          <dd>
            <p>A unique identifer of the face.
            If the same face can be found in successive frames,
            id is set to the same value for the face in all frames.
            The special value of zero indicates that the face is not tracked
            and several distinct faces can have the id of zero.
            Typically this means that the face detection engine does not
            support face tracking.</p>
          </dd>
          <dt><dfn><code>probability</code></dfn> of type <span class="idlMemberType">{{float}}</span></dt>
          <dd>
            <p>A confidence value in range [0,1].
            The approximate probability of the detected face being really
            a human face.
            The special value of exact zero indicates that the probability is
            not estimated or known.</p>
          </dd>
          <dt><dfn><code>contour</code></dfn> of type <span class="idlMemberType">{{FrozenArray}}&lt;{{Point2D}}&gt;</span></dt>
          <dd>
            <p>A contour surrounding the detected face.
            An example of a valid case is a four-point rectangle aligned to
            the image axes which is the bounding box supported in many
            platforms.
            However, the API does not guarantee that the returned data is
            anyhow aligned to image axes.
            The points are given in image coordinates.</p>
            <p>If the current {{MediaTrackSettings/faceDetectionMode}} setting
            of the {{MediaStreamTrack}} object is not
            {{FaceDetectionMode/"contour"}} or {{FaceDetectionMode/"mesh"}},
            then this member will not exist.</p>
            <p>The length of the array is controlled by the current
            {{MediaTrackSettings/faceDetectionNumContourPoints}} setting of
            the {{MediaStreamTrack}} object.</p>
          </dd>
          <dt><dfn><code>mesh</code></dfn> of type <span class="idlMemberType">{{FrozenArray}}&lt;{{Point2D}}&gt;</span></dt>
          <dd>
            <p>Arbitrary points on the face.
            When only a single point is returned,
            it SHOULD be located at the center of the face.
            The points are given in image coordinates.</p>
            <p>If the current {{MediaTrackSettings/faceDetectionMode}} setting
            of the {{MediaStreamTrack}} object is not
            {{FaceDetectionMode/"mesh"}},
            then this member will not exist.</p>
          </dd>
          <dt><dfn><code>landmarks</code></dfn> of type <span class="idlMemberType">{{FrozenArray}}&lt;{{DetectedFaceLandmark}}&gt;</span></dt>
          <dd>
            <p>A series of features of interest related to the detected
            face.</p>
            <p>If the current {{MediaTrackSettings/faceDetectionLandmarks}}
            setting of the {{MediaStreamTrack}} object is not
            <code>true</code>,
            then this member will not exist.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{DetectedFaceLandmark}}</h3>
<pre class="idl"
>dictionary DetectedFaceLandmark {
  required FrozenArray&lt;Point2D&gt; contour;
  FaceLandmark                  type;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{DetectedFaceLandmark}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="DetectedFaceLandmark" data-link-for="DetectedFaceLandmark">
          <dt><dfn><code>contour</code></dfn> of type <span class="idlMemberType">{{FrozenArray}}&lt;{{Point2D}}&gt;</span></dt>
          <dd>
            <p>A point at the center of the detected landmark, or
            a sequence of points defining the vertices of a simple polygon
            surrounding the landmark in either a clockwise or counter-clockwise
            direction.</p>
            <p>The length of the array is controlled by the current
            {{MediaTrackSettings/faceDetectionNumLandmarkPoints}} setting of
            the {{MediaStreamTrack}} object.</p>
          </dd>
          <dt><dfn><code>type</code></dfn> of type <span class="idlMemberType">{{FaceLandmark}}</span></dt>
          <dd><p>The type of the detected landmark.</p></dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{FaceLandmark}}</h3>
<pre class="idl"
>enum FaceLandmark {
  "eye",
  "eyeLeft",
  "eyeRight",
  "mouth",
  "nose"
};</pre>
      <section class="notoc">
        <h4>{{FaceLandmark}} Enumeration Description</h4>
        <dl data-dfn-for="FaceLandmark" data-link-for="FaceLandmark">
          <dt><dfn><code>eye</code></dfn></dt>
          <dd>
            <p>The landmark is identified as a human eye,
            either left or right.</p>
          </dd>
          <dt><dfn><code>eyeLeft</code></dfn></dt>
          <dd><p>The landmark is identified as a human left eye.</p></dd>
          <dt><dfn><code>eyeRight</code></dfn></dt>
          <dd><p>The landmark is identified as a human right eye.</p></dd>
          <dt><dfn><code>mouth</code></dfn></dt>
          <dd><p>The landmark is identified as a human mouth.</p></dd>
          <dt><dfn><code>nose</code></dfn></dt>
          <dd><p>The landmark is identified as a human nose.</p></dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{MediaTrackSupportedConstraints}}</h3>
<pre class="idl"
>partial dictionary MediaTrackSupportedConstraints {
  boolean faceDetectionMode = true;
  boolean faceDetectionLandmarks = true;
  boolean faceDetectionMaxNumFaces = true;
  boolean faceDetectionNumContourPoints = true;
  boolean faceDetectionNumLandmarkPoints = true;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{MediaTrackSupportedConstraints}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="MediaTrackSupportedConstraints" data-link-for="MediaTrackSupportedConstraints">
          <dt><dfn><code>faceDetectionMode</code></dfn> of type <span class="idlMemberType">{{boolean}}</span>, defaulting to <code>true</code></dt>
          <dd>
            <p>Whether <a>face detection mode</a> constraining is
            recognized.</p>
          </dd>
          <dt><dfn><code>faceDetectionLandmarks</code></dfn> of type <span class="idlMemberType">{{boolean}}</span>, defaulting to <code>true</code></dt>
          <dd>
            <p>Whether <a>face landmark detection mode</a> constraining is
            recognized.</p>
          </dd>
          <dt><dfn><code>faceDetectionMaxNumFaces</code></dfn> of type <span class="idlMemberType">{{boolean}}</span>, defaulting to <code>true</code></dt>
          <dd>
            <p>Whether <a>maximum number of face detection faces</a>
            constraining is recognized.</p>
          </dd>
          <dt><dfn><code>faceDetectionNumContourPoints</code></dfn> of type <span class="idlMemberType">{{boolean}}</span>, defaulting to <code>true</code></dt>
          <dd>
            <p>Whether <a>number of face detection contour points</a>
            constraining is recognized.</p>
          </dd>
          <dt><dfn><code>faceDetectionNumLandmarkPoints</code></dfn> of type <span class="idlMemberType">{{boolean}}</span>, defaulting to <code>true</code></dt>
          <dd>
            <p>Whether <a>number of face detection landmark points</a>
            constraining is recognized.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{MediaTrackCapabilities}}</h3>
      <pre class="idl"
>partial dictionary MediaTrackCapabilities {
  sequence&lt;DOMString&gt; faceDetectionMode;
  sequence&lt;boolean&gt;   faceDetectionLandmarks;
  ULongRange          faceDetectionMaxNumFaces;
  ULongRange          faceDetectionNumContourPoints;
  ULongRange          faceDetectionNumLandmarkPoints;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{MediaTrackCapabilities}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="MediaTrackCapabilities" data-link-for="MediaTrackCapabilities">
          <dt><dfn><code>faceDetectionMode</code></dfn> of type <span class="idlMemberType">sequence&lt;{{DOMString}}&gt;</span></dt>
          <dd>
            <p>A sequence of supported <a>face detection modes</a>.
            Each string MUST be one of the members of
            {{FaceDetectionMode}}.</p>
          </dd>
          <dt><dfn><code>faceDetectionLandmarks</code></dfn> of type <span class="idlMemberType">sequence&lt;{{boolean}}&gt;</span></dt>
          <dd>
            <p>A sequence of supported <a>face landmark detection modes</a>.
            If the source cannot do landmark detection,
            a single <code>false</code> is reported.
            If the landmark detection cannot be turned off,
            a single <code>true</code> is reported.
            If the script can control the detection,
            both <code>false</code> and <code>true</code> are reported as
            possible values.</p>
          </dd>
          <dt><dfn><code>faceDetectionMaxNumFaces</code></dfn> of type <span class="idlMemberType">{{ULongRange}}</span></dt>
          <dd>
            <p>A supported range for the <a>maximum number of face detection
            faces</a>.</p>
          </dd>
          <dt><dfn><code>faceDetectionNumContourPoints</code></dfn> of type <span class="idlMemberType">{{ULongRange}}</span></dt>
          <dd>
            <p>A supported range for the <a>number of face detection contour
            points</a>.</p>
          </dd>
          <dt><dfn><code>faceDetectionNumLandmarkPoints</code></dfn> of type <span class="idlMemberType">{{ULongRange}}</span></dt>
          <dd>
            <p>A supported range for the <a>number of face detection landmark
            points</a>.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{MediaTrackConstraintSet}}</h3>
<pre class="idl"
>partial dictionary MediaTrackConstraintSet {
  ConstrainDOMString faceDetectionMode;
  ConstrainBoolean   faceDetectionLandmarks;
  ConstrainULong     faceDetectionMaxNumFaces;
  ConstrainULong     faceDetectionNumContourPoints;
  ConstrainULong     faceDetectionNumLandmarkPoints;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{MediaTrackConstraintSet}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="MediaTrackConstraintSet" data-link-for="MediaTrackConstraintSet">
          <dt><dfn><code>faceDetectionMode</code></dfn> of type <span class="idlMemberType">{{ConstrainDOMString}}</span></dt>
          <dd>
            <p>The string MUST be one of the members of {{FaceDetectionMode}}.
            See <a>face detection mode</a> constrainable property.</p>
          </dd>
          <dt><dfn><code>faceDetectionLandmarks</code></dfn> of type <span class="idlMemberType">{{ConstrainBoolean}}</span></dt>
          <dd>
            <p>See <a>face landmark detection mode</a> constrainable property.</p>
          </dd>
          <dt><dfn><code>faceDetectionMaxNumFaces</code></dfn> of type <span class="idlMemberType">{{ConstrainULong}}</span></dt>
          <dd>
            <p>See <a>maximum number of face detection faces</a> constrainable property.</p>
          </dd>
          <dt><dfn><code>faceDetectionNumContourPoints</code></dfn> of type <span class="idlMemberType">{{ConstrainULong}}</span></dt>
          <dd>
            <p>See <a>number of face detection contour points</a> constrainable property.</p>
          </dd>
          <dt><dfn><code>faceDetectionNumLandmarkPoints</code></dfn> of type <span class="idlMemberType">{{ConstrainULong}}</span></dt>
          <dd>
            <p>See <a>number of face detection landmark points</a> constrainable property.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{MediaTrackSettings}}</h3>
<pre class="idl"
>partial dictionary MediaTrackSettings {
  DOMString faceDetectionMode;
  boolean   faceDetectionLandmarks;
  long      faceDetectionMaxNumFaces;
  long      faceDetectionNumContourPoints;
  long      faceDetectionNumLandmarkPoints;
};</pre>
      <section class="notoc">
        <h4>Dictionary {{MediaTrackSettings}} Members</h4>
        <dl class="dictionary-members" data-dfn-for="MediaTrackSettings" data-link-for="MediaTrackSettings">
          <dt><dfn><code>faceDetectionMode</code></dfn> of type <span class="idlMemberType">{{DOMString}}</span></dt>
          <dd>
            <p>Current <a>face detection mode</a> setting.
            The string MUST be one of the members of {{FaceDetectionMode}}.</p>
          </dd>
          <dt><dfn><code>faceDetectionLandmarks</code></dfn> of type <span class="idlMemberType">{{boolean}}</span></dt>
          <dd>
            <p>Current <a>face landmark detection mode</a> setting.</p>
          </dd>
          <dt><dfn><code>faceDetectionMaxNumFaces</code></dfn> of type <span class="idlMemberType">{{long}}</span></dt>
          <dd>
            <p>Current <a>maximum number of face detection faces</a> setting.</p>
          </dd>
          <dt><dfn><code>faceDetectionNumContourPoints</code></dfn> of type <span class="idlMemberType">{{long}}</span></dt>
          <dd>
            <p>Current <a>number of face detection contour points</a> setting.</p>
          </dd>
          <dt><dfn><code>faceDetectionNumLandmarkPoints</code></dfn> of type <span class="idlMemberType">{{long}}</span></dt>
          <dd>
            <p>Current <a>number of face detection landmark points</a> setting.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>{{FaceDetectionMode}}</h3>
<pre class="idl"
>enum FaceDetectionMode {
  "none",
  "presence",
  "contour",
  "mesh"
};</pre>
      <section class="notoc">
        <h4>{{FaceDetectionMode}} Enumeration Description</h4>
        <dl data-dfn-for="FaceDetectionMode" data-link-for="FaceDetectionMode">
          <dt><dfn><code>none</code></dfn></dt>
          <dd>
            <p>This source does not offer human face detection.
            For setting, this is interpreted as a command to turn of
            the detection.</p>
          </dd>
          <dt><dfn><code>presence</code></dfn></dt>
          <dd>
            <p>This source offers human face presence detection,
            or such a mode is requested.</p>
            <p class="note">This mode may be useful with a <code>true</code>
            <a>face landmark detection mode</a> in order to detect human face
            landmarks but not contours or meshes.</p>
          </dd>
          <dt><dfn><code>contour</code></dfn></dt>
          <dd>
            <p>This source offers human face contour detection,
            or such a mode is requested.</p>
          </dd>
          <dt><dfn><code>mesh</code></dfn></dt>
          <dd>
            <p>This source offers human face mesh and contour detection,
            or such a mode is requested.</p>
            <p class="note">It is possible to disable human face contour
            detection in this mode by setting the <a>number of face detection
            contour points</a> to zero.</p>
          </dd>
        </dl>
      </section>
    </section>
    <section>
      <h3>Constrainable Properties</h3>
      <ol>
        <li>
          <p><dfn>Face detection mode</dfn> describes which face details
          (presence, contour points, mesh points) are to be detected.</p>
        </li>
        <li>
          <p><dfn>Face landmark detection mode</dfn> describes whether human
          face landmarks are to be detected and exposed.</p>
        </li>
        <li>
          <p><dfn>Maximum number of face detection faces</dfn> descibes how
          many human faces are to be detected and exposed at most.</p>
        </li>
        <li>
          <p><dfn>Number of face detection contour points</dfn> descibes how
          many human faces contour points are to be detected and exposed per
          human face.</p>
        </li>
        <li>
          <p><dfn>Number of face detection landmark points</dfn> descibes how
          many human faces landmark points are to be detected and exposed per
          human face landmark.</p>
        </li>
      </ol>
    </section>
    <section>
      <h3>Examples</h3>
      <pre class="example">
// main.js:
// Check if face detection is supported by the browser
const supports = navigator.mediaDevices.getSupportedConstraints();
if (supports.faceDetectionMode &amp;&amp;
    supports.faceDetectionNumContourPoints) {
  // Browser supports face contour detection.
} else {
  throw('Face contour detection is not supported');
}

// Open camera with face detection enabled
const stream = await navigator.mediaDevices.getUserMedia({
  video: {
    faceDetectionMode: 'contour',
    faceDetectionNumContourPoints: {exact: 4}
  }
});
const [videoTrack] = stream.getVideoTracks();

// Use a video worker and show to user.
const videoElement = document.querySelector('video');
const videoWorker = new Worker('video-worker.js');
videoWorker.postMessage({track: videoTrack}, [videoTrack]);
const {data} = await new Promise(r => videoWorker.onmessage);
videoElement.srcObject = new MediaStream([data.videoTrack]);

// video-worker.js:
self.onmessage = async ({data: {track}}) => {
  const generator = new VideoTrackGenerator();
  parent.postMessage({videoTrack: generator.track}, [generator.track]);
  const {readable} = new MediaStreamTrackProcessor({track});
  const transformer = new TransformStream({
    async transform(frame, controller) {
      for (const face of frame.detectedFaces) {
        console.log(
          `Face @ (${face.contour[0].x}, ${face.contour[0].y}), ` +
                 `(${face.contour[1].x}, ${face.contour[1].y}), ` +
                 `(${face.contour[2].x}, ${face.contour[2].y}), ` +
                 `(${face.contour[3].x}, ${face.contour[3].y})`);
      }
      controller.enqueue(frame);
    }
  });
  await readable.pipeThrough(transformer).pipeTo(generator.writable);
};
      </pre>
    </section>
  </section>
</body>
</html>
